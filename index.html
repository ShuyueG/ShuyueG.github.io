<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
        <link rel="shortcut icon" href="imgs/myIcon.jpg" />
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <meta name="keywords" content="Shuyue Guan, Guan Shuyue, BME, Machine Learning, Computer Vision, The George Washington University" />
        <meta name="description" content="Shuyue Guan's home page" />
        <link rel="stylesheet" href="./css/jemdoc.css" type="text/css" />
    <link rel="stylesheet" href="./css/full_img.css" type="text/css" />
    <link rel="stylesheet" href="./css/zoom_img.css" type="text/css" />
    <link rel="stylesheet" href="./css/abstract.css" type="text/css" />

        <title>Shuyue Guan</title>
        <script type="text/javascript">
//<![CDATA[
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-99569700-1', 'auto');
        ga('send', 'pageview');
        //]]>
        </script>
    

</head>
<body>
        <div id="layout-content" style="margin-top:25px">
                <table cellspacing="0">
                        <tbody>
                                <tr>
                                        <td width="1000">
                                                
                                            <table width="100%" cellspacing="0">
          <tbody>
                                <tr>
                                        <td width="44%" height="75"  id="titlename"><div id="toptitle"><h1>Shuyue (Frank) Guan </h1></div></td>
                                        <td id="hide"  width="54%"><img src="imgs/name.png" height="46" style="margin-bottom:-12px" /></td>
                                </tr>
    </tbody>
          </table>
                                          

                                          <p style="text-align:justify;">I am a Ph.D. candidate in&nbsp;Department of Biomedical Engineering,&nbsp;the George Washington University, working at&nbsp;the <a href="https://loewlab.seas.gwu.edu/" target="_blank">Medical Imaging &amp; Image Analysis Laboratory</a>. <br />
                                            My advisor is&nbsp;<a href="https://www.seas.gwu.edu/murray-h-loew" target="_blank">Murray Loew</a>.</p>
                                              
                                          
                                          <p style="text-align:justify;">My primary research interests lie in machine learning, image processing, and computer vision. <br />
                                            I recently focus on the data&nbsp;separability measure, learnability for deep learning models and&nbsp;the applications of machine learning&nbsp;to solve problems in the field of image analysis, especially, for the medical imaging.&nbsp;My previous studies were about the non-destructive testing of wood composite panel internal defect in the <a href="https://en.nefu.edu.cn/disp.php?sn=18852" target="_blank">Biophysics Program</a>, Northeast Forestry University, China.</p>



                                        </td>

                                        <td width="60">
                                        </td>

                                        <td><center>
                                          <img src="imgs/bio-photo.png" border="0" style="margin-top:0px" width="180" /><br />
                                        </center>
                                          <!-- <center>This pencil sketch is generated </br> by this <a href="http://www.cse.cuhk.edu.hk/leojia/projects/pencilsketch/pencil_drawing.htm">algorithm</a>.</center> --></td>
</tr>
                        </tbody>
                </table>


            <!--    
            <a href="doc/old_CV.pdf">Curriculum Vitae (by 2016)</a><br />
            -->
                <br />Email: frankshuyueguan AT gwu DOT edu
                <table width="100%" cellspacing="0">
                  <tbody>
                    <tr>
                      <td>&nbsp;</td>
                    </tr>
                  </tbody>
                </table>
<table width="100%" cellspacing="0">
          <tbody>
                                <tr>
                                        <td width="6%">
                                                <center>
                                                        <a href="https://scholar.google.com/citations?user=F0ABc9cAAAAJ&hl=en"><img src="imgs/gscholar_icon.png" border="0" style="margin-top:0px" height="45" width="45" /></a><br />
                                                </center>
                                        </td><!-- <td width="60" /> -->


                                        <td width="6%">
                                                <center>
                                                  <a href="https://www.researchgate.net/profile/Shuyue_Guan"><img src="imgs/researchgate_icon.png" border="0" style="margin-top:0px" height="45" width="45" /></a><br />
                                                </center>
                                        </td>

                                        <td width="6%">
                                                <center>
                                                  <a href="https://github.com/ShuyueG"><img src="imgs/github_icon.png" border="0" style="margin-top:0px" height="45" width="45" /></a><br />
                                                </center>
                                        </td>

                                        <td width="6%">
                                            <center>
                                            <a href="https://www.linkedin.com/in/shuyue-guan-813182127/"><img src="imgs/linkedin_icon.png" border="0" style="margin-top:0px" height="45" width="45" /></a><br />
                                          </center>
                                        </td>
                                        <td width="6%">&nbsp;</td>

                                        <td width="70%" align="right"><br />
                                        <i>"You the wise, tell me, why should our days leave us, never to return?"</i> - Ziqing Zhu</td>
                                </tr>
    </tbody>
          </table>



                <div class="table-responsive">
                        <h2 id="news">
                        </h2>
                </div>


                <table id="navigate">
                        <thead role="rowgroup">
     
                        </thead>
                </table>
        </div>




<h2 id="publication">Recent Projects &amp; Publications</h2>



<div class="areatitle">Transparent Deep Learning</div>
    
      <table width="961" border="0" cellpadding="2" cellspacing="10">
          
          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_icmla2020.png" alt="Adversarial examples generated by pairs of data" width="180px" />
                                        </div>      
                                                                        
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Analysis of Generalizability of Deep Neural Networks Based on the Complexity of Decision Boundary</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        [<em>In press</em>] International Conference on Machine Learning and Applications (ICMLA), 2020<br />
                            [<a href="https://arxiv.org/abs/2009.07974" target="_blank">Arxiv</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>For supervised learning models, the analysis of generalization ability (generalizability) is vital because the generalizability expresses how well a model will perform on unseen data. Traditional generalization methods, such as the VC dimension, do not apply to deep neural network (DNN) models. Thus, new theories to explain the generalizability of DNNs are required. In this study, we hypothesize that the DNN with a simpler decision boundary has better generalizability by the law of parsimony (Occam's Razor). <b>We create the decision boundary complexity (DBC) score to define and measure the complexity of decision boundary of DNNs.</b> The idea of the DBC score is to generate data points (called adversarial examples) on or near the decision boundary. Our new approach then measures the complexity of the boundary using the entropy of eigenvalues of these data. The method works equally well for high-dimensional data. We use training data and the trained model to compute the DBC score. And, the ground truth for model's generalizability is its test accuracy. Experiments based on the DBC score have verified our hypothesis. The DBC is shown to provide an effective method to measure the complexity of a decision boundary and gives a quantitative measure of the generalizability of DNNs.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->

                          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_ictai2020.png" alt="Two clusters (classes) dataset with different label assignments" width="180px" />
                                        </div>      
                                                                        
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>An Internal Cluster Validity Index Based on Distance-based Separability Measure</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        [<em>In press</em>] International Conference on Tools with Artificial Intelligence (ICTAI), 2020<br />
                            [<a href="https://arxiv.org/abs/2009.01328" target="_blank">Arxiv</a>] [<a href="https://github.com/ShuyueG/CVI_using_DSI" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>To evaluate clustering results is a significant part in cluster analysis. Usually, there is no true class labels for clustering as a typical unsupervised learning. Thus, a number of internal evaluations, which use predicted labels and data, have been created. They also named internal cluster validity indices (CVIs). Without true labels, to design an effective CVI is not simple because it is similar to create a clustering method. And, to have more CVIs is crucial because there is no universal CVI that can be used to measure all datasets, and no specific method for selecting a proper CVI for clusters without true labels. Therefore, to apply more CVIs to evaluate clustering results is necessary. In this paper, <b>we propose a novel CVI - called Distance-based Separability Index (DSI), based on a data separability measure</b>. We applied the DSI and eight other internal CVIs including early studies from Dunn (1974) to most recent studies CVDD (2019) as comparison. We used an external CVI as ground truth for clustering results of five clustering algorithms on 12 real and 97 synthetic datasets. Results show DSI is an effective, unique, and competitive CVI to other compared CVIs. In addition, we summarized the general process to evaluate CVIs and created a new method - rank difference - to compare the results of CVIs.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->

                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_dsi.png" alt="Complexity measures for two-class datasets with different cluster SDs" width="180px" />
                                        </div>      
                                                                        
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Data Separability for Neural Network Classifiers and the Development of a Separability Index</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew, Hanseok Ko<br />
        (<em>Preprint</em>), 2020<br />
                            [<a href="https://arxiv.org/abs/2005.13120" target="_blank">Arxiv</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p> In machine learning, the performance of a classifier depends on both the classifier model and the dataset. For a specific neural network classifier, the training process varies with the training set used; some training data make training accuracy fast converged to high values, while some data may lead to slowly converged to lower accuracy. To quantify this phenomenon, <b>we created the Distance-based Separability Index (DSI), which is independent of the classifier model, to measure the separability of datasets</b>. In this paper, we consider the situation where different classes of data are mixed together in the same distribution is most difficult for classifiers to separate, and we show that the DSI can indicate whether data belonging to different classes have similar distributions. When comparing our proposed approach with several existing separability/complexity measures using synthetic and real datasets, the results show the DSI is an effective separability measure. We also discussed possible applications of the DSI in the fields of data science, machine learning, and deep learning.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->

                          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_GANmeasureLS.png" alt="Real and generated datasets from virtual GANs on MNIST" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>A Novel Measure to Evaluate Generative Adversarial Networks Based on Direct Analysis of Generated Images</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        <em>(Preprint)</em>, 2020<br />
                            [<a href="https://arxiv.org/abs/2002.12345" target="_blank">Arxiv</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>The Generative Adversarial Network (GAN) is a state-of-the-art technique in the field of deep learning. A number of recent papers address the theory and applications of GANs in various fields of image processing. Fewer studies, however, have directly evaluated GAN outputs. Those that have been conducted focused on using classification performance (e.g., Inception Score) and statistical metrics (e.g., Fréchet Inception Distance). Here, we consider a fundamental way to evaluate GANs by directly analyzing the images they generate, instead of using them as inputs to other classifiers. We characterize the performance of a GAN as an image generator according to three aspects: 1) Creativity: non-duplication of the real images. 2) Inheritance: generated images should have the same style, which retains key features of the real images. 3) Diversity: generated images are different from each other. A GAN should not generate a few different images repeatedly. Based on the three aspects of ideal GANs, <b>we have designed the Likeness Score (LS) to evaluate GAN performance</b>, and have applied it to evaluate several typical GANs. We compared our proposed measure with three commonly used GAN evaluation methods: Inception Score (IS), Fréchet Inception Distance (FID) and 1-Nearest Neighbor classifier (1NNC). In addition, we discuss how these evaluations could help us deepen our understanding of GANs and improve their performance.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
                          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_GANmeasure.png" alt="Problems of generated images from the perspective of distribution" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Evaluation of Generative Adversarial Network Performance Based on Direct Analysis of Generated Images</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2019<br />
                            [<a href="https://doi.org/10.1109/AIPR47015.2019.9174595" target="_blank">Paper</a>] [<a href="doc/AIPR2019.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Recently, a number of papers have addressed the theory and applications of the Generative Adversarial Network (GAN) in various fields of image processing. Fewer studies, however, have directly evaluated GAN outputs. Those that have been conducted focused on using classification performance and statistical metrics. In this paper, we consider a fundamental way to evaluate GANs by directly analyzing the images they generate, instead of using them as inputs to other classifiers. <b>We consider an ideal GAN according to three aspects</b>: 1) Creativity: non-duplication of the real images. 2) Inheritance: generated images should have the same style, which retains key features of the real images. 3) Diversity: generated images are different from each other. <b>Based on the three aspects, we have designed the Creativity-Inheritance-Diversity (CID) index to evaluate GAN performance</b>. We compared our proposed measures with three commonly used GAN evaluation methods: Inception Score (IS), Fréchet Inception Distance (FID) and 1-Nearest Neighbor classifier (1NNC). In addition, we discuss how the evaluation could help us deepen our understanding of GANs and improve their performance.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
          
    </table>

<div class="areatitle">Deep Learning Applications on Medical Images</div>
    
          <table width="961" border="0" cellpadding="2" cellspacing="10">
                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_dcunet.png" alt="The DC-Block" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>DC-UNet: Rethinking the U-Net Architecture with Dual Channel Efficient CNN for Medical Images Segmentation</papertitle><br />
        
        Ange Lou, <b>Shuyue Guan</b>, Murray Loew<br />
        (<em>Preprint</em>), 2020<br />
                            [<a href="https://arxiv.org/abs/2006.00414" target="_blank">Arxiv</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p> Recently, deep learning has become much more popular in computer vision area. The Convolution Neural Network (CNN) has brought a breakthrough in images segmentation areas, especially, for medical images. In this regard, U-Net is the predominant approach to medical image segmentation task. The U-Net not only performs well in segmenting multimodal medical images generally, but also in some tough cases of them. However, we found that the classical U-Net architecture has limitation in several aspects. Therefore, we applied modifications: 1) designed efficient CNN architecture to replace encoder and decoder, 2) applied residual module to replace skip connection between encoder and decoder to improve based on the-state-of-the-art U-Net model. Following these modifications, we designed a novel architecture by adding Dual-Channel blocks in the U-Net model, called Dual Channel U-Net (DC-UNet), as a potential successor to the U-Net architecture. We created a new effective CNN architecture and build the DC-UNet based on this CNN. We have evaluated our model on three datasets with tough cases and have obtained a relative improvement in performance of 2.90%, 1.49% and 11.42% respectively compared with classical U-Net, especially, DC-UNet has about 30% parameters of the U-Net. In addition, we introduced the Tanimoto similarity and used it for gray-to-gray image comparisons instead of the Jaccard similarity.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                                    <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_spie2019.png" alt="The flowchart of our experiment plan" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Using generative adversarial networks and transfer learning for breast cancer detection by convolutional neural networks</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        SPIE Medical Imaging, 2019<br />
                            [<a href="https://doi.org/10.1117/12.2512671" target="_blank">Paper</a>] [<a href="doc/SPIE2019.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>In the U.S., breast cancer is diagnosed in about 12% of women during their lifetime and it is the second leading reason for women’s death. Since early diagnosis could improve treatment outcomes and longer survival times for breast cancer patients, it is significant to develop breast cancer detection techniques. The Convolutional Neural Network (CNN) can extract features from images automatically and then perform classification. To train the CNN from scratch, however, requires a large number of labeled images, which is infeasible for some kinds of medical image data such as mammographic tumor images. In this paper, we proposed two solutions to the lack of training images. 1)To generate synthetic mammographic images for training by the Generative Adversarial Network (GAN). Adding GAN generated images made to train CNN from scratch successful and adding more GAN images improved CNN’s validation accuracy to at most (best) 98.85%. 2)To apply transfer learning in CNN. We used the pre-trained VGG-16 model to extract features from input mammograms and used these features to train a Neural Network (NN)-classifier. The stable average validation accuracy converged at about 91.48% for classifying abnormal vs. normal cases in the DDSM database. Then, we combined the two deep-learning based technologies together. That is to apply GAN for image augmentation and transfer learning in CNN for breast cancer detection. To the training set including real and GAN augmented images, although transfer learning model did not perform better than the CNN, the speed of training transfer learning model was about 10 times faster than CNN training. Adding GAN images can help training avoid over-fitting and image augmentation by GAN is necessary to train CNN classifiers from scratch. On the other hand, transfer learning is necessary to be applied for training on pure real images. To apply GAN to augment training images for training CNN classifier obtained the best classification performance.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_jmi2019.png" alt="Training accuracy and validation accuracy for training datasets" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>
                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Breast cancer detection using synthetic mammograms from generative adversarial networks in convolutional neural networks</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        Journal of Medical Imaging (JMI), 2019<br />
                            [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6430964/" target="_blank">Paper</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p> The convolutional neural network (CNN) is a promising technique to detect breast cancer based on mammograms. Training the CNN from scratch, however, requires a large amount of labeled data. Such a requirement usually is infeasible for some kinds of medical image data such as mammographic tumor images. Because improvement of the performance of a CNN classifier requires more training data, the creation of new training images, image augmentation, is one solution to this problem. We applied the generative adversarial network (GAN) to generate synthetic mammographic images from the digital database for screening mammography (DDSM). From the DDSM, we cropped two sets of regions of interest (ROIs) from the images: normal and abnormal (cancer/tumor). Those ROIs were used to train the GAN, and the GAN then generated synthetic images. For comparison with the affine transformation augmentation methods, such as rotation, shifting, scaling, etc., we used six groups of ROIs [three simple groups: affine augmented, GAN synthetic, real (original), and three mixture groups of any two of the three simple groups] for each to train a CNN classifier from scratch. And, we used real ROIs that were not used in training to validate classification outcomes. Our results show that, to classify the normal ROIs and abnormal ROIs from DDSM, adding GAN-generated ROIs in the training data can help the classifier prevent overfitting, and on validation accuracy, the GAN performs about 3.6% better than affine transformations for image augmentation. Therefore, GAN could be an ideal augmentation approach. The images augmented by GAN or affine transformation cannot substitute for real images to train CNN classifiers because the absence of real images in the training set will cause over-fitting.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
              
           <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_aipr2018.png" alt="Segmentation results of one patient" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>
                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Segmentation of Thermal Breast Images Using Convolutional and Deconvolutional Neural Networks</papertitle><br />
        
        <b>Shuyue Guan</b>, Nada Kamona, Murray Loew<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2018<br />
                            [<a href="https://doi.org/10.1109/AIPR.2018.8707379" target="_blank">Paper</a>] [<a href="doc/AIPR2018.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Breast cancer is the second leading cause of death for women in the U.S. Early detection of breast cancer has been shown to be the key to higher survival rates for breast cancer patients. We are investigating infrared thermography as a noninvasive adjunctive to mammography for breast screening. Thermal imaging is safe, radiation-free, pain-free, and non-contact. Segmentation of breast area from the acquired thermal images will help limit the area for tumor search and reduce the time and effort needed for manual hand segmentation. Autoencoder-like convolutional and deconvolutional neural networks (C-DCNN) are promising computational approaches to automatically segment breast areas in thermal images. In this study, we apply the C-DCNN to segment breast areas from our thermal breast images database, which we are collecting in our clinical trials by imaging breast cancer patients with our infrared camera (N2 Imager). For training the C-DCNN, the inputs are 132 gray-value thermal images and the corresponding manually-cropped breast area images (binary masks to designate the breast areas). For testing, we input thermal images to the trained C-DCNN and the output after post-processing are the binary breast-area images. Cross-validation and comparison with the ground-truth images show that the C-DCNN is a promising method to segment breast areas. The results demonstrate the capability of C-DCNN to learn essential features of breast regions and delineate them in thermal images.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
                
           <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_aipr2017.png" alt="Comparing of the three CNN classification models" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>
                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Breast Cancer Detection Using Transfer Learning in Convolutional Neural Networks</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2017<br />
                            [<a href="https://doi.org/10.1109/AIPR.2017.8457948" target="_blank">Paper</a>] [<a href="doc/AIPR2017.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>In the U.S., breast cancer is diagnosed in about 12 % of women during their lifetime and it is the second leading reason for women's death. Since early diagnosis could improve treatment outcomes and longer survival times for breast cancer patients, it is significant to develop breast cancer detection techniques. The Convolutional Neural Network (CNN) can extract features from images automatically and then perform classification. To train the CNN from scratch, however, requires a large number of labeled images, which is infeasible for some kinds of medical image data such as mammographic tumor images. A promising solution is to apply transfer learning in CNN. In this paper, we firstly tested three training methods on the MIAS database: 1) trained a CNN from scratch, 2) applied the pre-trained VGG-16 model to extract features from input mammograms and used these features to train a Neural Network (NN)-classifier, 3) updated the weights in several final layers of the pre-trained VGG-16 model by back-propagation (fine-tuning) to detect abnormal regions. We found that method 2) is ideal for study because the classification accuracy of fine-tuning model was just 0.008 higher than that of feature extraction model but time cost of feature extraction model was only about 5% of that of the fine-tuning model. Then, we used method 2) to classify regions: benign vs. normal, malignant vs. normal and abnormal vs. normal from the DDSM database with 10-fold cross validation. The average validation accuracy converged at about 0.905 for abnormal vs. normal cases, and there was no obvious overfitting. This study shows that applying transfer learning in CNN can detect breast cancer from mammograms, and training a NN-classifier by feature extraction is a faster method in transfer learning.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
    </table>

        <h2 id="preprints">Past Researches</h2>
    
    
<div class="areatitle">Hyperspectral Images-based Cardiac Ablation Lesion Detection</div>
    
              <table width="961" border="0" cellpadding="2" cellspacing="10">
                  
                  
                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_jmi2018.png" alt="Results for porcine atria clustered by k-means" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Application of unsupervised learning to hyperspectral imaging of cardiac ablation lesions</papertitle><br />
        
        <b>Shuyue Guan</b>, Huda Asfour, Narine Sarvazyan, Murray Loew<br />
        Journal of Medical Imaging (JMI), 2018<br />
                            [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6294845/" target="_blank">Paper</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p> Atrial fibrillation is the most common cardiac arrhythmia. It is being effectively treated using the radiofrequency
ablation (RFA) procedure, which destroys culprit tissue and creates scars that prevent the spread of
abnormal electrical activity. Long-term success of RFA could be improved further if ablation lesions can be
directly visualized during the surgery. We have shown that autofluorescence-based hyperspectral imaging
(aHSI) can help to identify lesions based on spectral unmixing. We show that use of k-means clustering,
an unsupervised learning method, is capable of detecting RFA lesions without a <em>priori</em> knowledge of the
lesions’ spectral characteristics. We also show that the number of spectral bands required for successful
lesion identification can be significantly reduced, enabling the use of increased spectral bandwidth.
Together, these findings can help with clinical implementation of a percutaneous aHSI catheter, since by
reducing the number of spectral bands one can reduce hypercube acquisition and processing times, and
by increasing the spectral width of individual bands one can collect more photons. The latter is of critical
importance in low-light applications such as intracardiac aHSI. The ultimate goal of our studies is to help
improve clinical outcomes for atrial fibrillation patients.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                                    <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_boe2018.png" alt="Histogram distributions and boxplot results of accuracies for all studies" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Optimization of wavelength selection for multispectral image acquisition: a case study of atrial ablation lesions</papertitle><br />
        
        Huda Asfour, <b>Shuyue Guan</b>, Narine Muselimyan, Luther Swift, Murray Loew, Narine Sarvazyan<br />
        Biomedical Optics Express, 2018<br />
                            [<a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-9-5-2189" target="_blank">Paper</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>In vivo autofluorescence hyperspectral imaging of moving objects can be challenging due to motion artifacts and to the limited amount of acquired photons. To address both limitations, we selectively reduced the number of spectral bands while maintaining accurate target identification. Several downsampling approaches were applied to data obtained from the atrial tissue of adult pigs with sites of radiofrequency ablation lesions. Standard image qualifiers such as the mean square error, the peak signal-to-noise ratio, the structural similarity index map, and an accuracy index of lesion component images were used to quantify the effects of spectral binning, an increased spectral distance between individual bands, as well as random combinations of spectral bands. Results point to several quantitative strategies for deriving combinations of a small number of spectral bands that can successfully detect target tissue. Insights from our studies can be applied to a wide range of applications.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                         
          
    </table>
    
    <div class="areatitle">Climate (Frost Point) Data Collection and Analysis by R</div>

              <table width="961" border="0" cellpadding="2" cellspacing="10">
                  
                  
                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/global_frost_point.png" alt="Frost points at 12Z o'clock in Mar 09, 2010" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">
<papertitle>Project: Frost Detection</papertitle><br />
        Advisor: <a href="https://www.cs.seas.gwu.edu/claire-monteleoni" target="_blank">Claire Monteleoni</a><br />
                            June-August, 2016<br />
                            
                            [<a href="doc/fp_proj/Report1.pdf" target="_blank">Report1</a>] [<a href="doc/fp_proj/Report2.pdf" target="_blank">Report2</a>] [<a href="doc/fp_proj/Report3.pdf" target="_blank">Report3</a>] [<a href="doc/fp_proj/Report_fin.pdf" target="_blank">ReportFIN</a>] [<a href="https://github.com/ShuyueG/frost_point" target="_blank">Code</a>]

<button class="collapsible">Flowchart</button>

<div class="content">
  <p> <img src="imgs/frost_point_fc.png" alt="Results for porcine atria clustered by k-means" width="180px" class="center" /></p>
        
</div>
                      
                    </td>
                </tr>
          <!-- paper end -->
                                    
          
    </table>
    
<div class="areatitle">Non-destructive Testing for Wooden Materials</div>
    
        
              <table width="961" border="0" cellpadding="2" cellspacing="10">
                  
                  
                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_fuzzyBPNN.png" alt="FBP Flow Chart" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Wood Defects Recognition Based on Fuzzy BP Neural Network</papertitle><br />
        
        Hongbo Mu, Mingming Zhang, Dawei Qi, <b>Shuyue Guan</b>, Haiming Ni<br />
        International Journal of Smart Home, 2015<br />
                            [<a href="https://www.earticle.net/Article/A246130" target="_blank">Paper</a>] [<a href="http://dx.doi.org/10.14257/ijsh.2015.9.5.14">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Firstly, we applied the X-ray non-destructive testing technology to detect wood defects for
getting the images. After graying the images, we calculated their GLCMS(Gray Level Cooccurrence Matrixes), then we normalized GLCMS to obtain the joint probabilities of GLCMS. The feature vectors of images, which included 13 eigenvalues of images were
calculated and extracted by the joint probability of GLCMS. The fuzzy BP neural
network(abbreviated as FBP) was designed by combining fuzzy mathematics and BP neural
network . And the FBP neural network was regarded as the membership function of feature
vectors, the outputs of the network was regarded as the degree of membership to the feature
vectors in each category. We use the maximum degree of membership method for the pattern
recognition of feature vectors, so the automatic identification and classification for feature
vectors were achieved , and then the automatic identification of wood defects was realized.
By simulated study and training many times, the results shown that the average recognition
success rate of the network was more than 90%, and some FBP networks had an extremely
high recognition success rate to training samples and test samples.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                                    <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_aiss2013.png" alt="The larger number of E demonstrates the poorer quality of
tested blockboards" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Defect Edge Detection in Blockboard X-ray Images by Shannon Entropy</papertitle><br />
        
        <b>Shuyue Guan</b>, Dawei Qi<br />
        Advances in Information Sciences and Service Sciences (AISS), 2013<br />
                            [<a href="/doc/AISS2013.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>A Shannon entropy-based image processing approach is introduced and applied to the blockboard
X-ray images obtained from nondestructive scanning. X-ray nondestructive testing technology has been
applied to the detection of internal defects in blockboard. In this paper, we select a probability
distribution function to calculate the Shannon entropy in images processing. And we define a novel
Defect Edge Index (DEI) for analyzing the defect edges in images. Through studying and processing
the DEIs, the defect edges extracting is achieved. Furthermore, a new index E for quality evaluation
was also studied out by the DEIs. The number of E can demonstrate the quality of examined
blockboard. Experimental results display that the image processing method based on Shannon entropy
theory is effective and the quality evaluation index E is accurate. Thus, a promising method for
detecting and analyzing defect edges in blockboard X-ray images is provided.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                                       <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_ijact2012.png" alt="Redrawing defect region by lines" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Defects description in blockboard by Hough transform and Minimum-Perimeter polygons</papertitle><br />
        
        <b>Shuyue Guan</b>, Dawei Qi<br />
        International Journal of Advancements in Computing Technology (IJACT), 2012<br />
                            [<a href="/doc/IJACT2012pdf.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>The blockboard defects were detected by Hough transform and Minimum-Perimeter Polygons. Xray
nondestructive testing system was used to obtain X-ray blockboard images. The rough binary
images of blockboard were got from X-ray images. An initial area of the blockboard defect was
simplified by Mathematical morphology, and then the edges of the area image were detected by
Minimum-Perimeter Polygons (MPP). Large numbers of lines around the boundary were detected by
Hough transform. The lines data were processed by the mathematics and computer graphics methods.
The results show that the blockboard defects can be described by several certain lines on the screen.
And then, the parameters of lines can be obtained by computers easily. Hence, to defects description,
the method in this paper is much more convenient and faster than the traditional methods.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->                      
                                        <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_aiss2012.png" alt="The Singularity Index &alpha; of Image" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Multifractal Analysis of Blockboard X-Ray Images for the Defect Detection</papertitle><br />
        
        <b>Shuyue Guan</b>, Dawei Qi<br />
        Advances in Information Sciences and Service Sciences (AISS), 2012<br />
                            [<a href="/doc/AISS2012.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Nowadays, nondestructive testing technology is a new subject that has gotten rapid development. Xray
nondestructive scanning technology has been applied to the detection of internal defects in
blockboard for the purpose of obtaining prior information that can be used to arrive at better
production quality. Since producers currently cannot see the inside of blockboard until its faces are
revealed by cutting. Therefore, the recognition of internal defects has become gradually significant.
The traditional Euclidean geometry is not proficient of describing different natural objects and
phenomena. In contrast, fractal geometry and its multifractal extension are new implements which can
be used for describing, processing and analyzing complex shapes and images. A method in blockboard
X-ray image defect detection based on multifractal theory was applied in this paper. The Lipschitz–H&#xF6;lder exponent &alpha; of image was computed first. Then its multifractal spectrum <em>f(&alpha;)</em> was calculated and different image points were classified by analysis of multifractal spectrum <em>&alpha;-f(&alpha;).</em> Experimental results showed that the method based on multifractal theory was effective to detect defects in
blockboard X-ray images. Due to the information of defect, the areas and boundaries was obtained
accurately by this method. Hence, in this paper, a dependable method by applying multifractal theory
in defect detection of blockboard X-ray images was provided.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->                      
<!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_amr2012.png" alt="Structure of our CT scanning system" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Application of computed tomography in wood-polymer composites density detection</papertitle><br />
        
        Yu Han, Dawei Qi, <b>Shuyue Guan</b><br />
        Advanced Materials Research (AMR), 2012<br />
                            [<a href="https://www.scientific.net/AMR.428.57" target="_blank">Paper</a>] [<a href="doc/AMR2012.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>CT technology was used in nondestructive testing procedure of Wood-plastic composite in
the paper as well as computes the CT number range of different Wood-plastic composite tomography
slices in statistic method. A fitting mathematical model between CT number and Wood-plastic
composite density was Calculated, because of the linear relationship exists between Wood-plastic
composite density and CT number. Hence, a new method in the nondestructive testing of
Wood-plastic composite density was provided.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->     
<!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_ibi2011.png" alt="Fiberboard CT number-density relationship graph" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Automatic Fiberboard Density Testing Based on Application of Computed Tomography</papertitle><br />
        
       <b>Shuyue Guan</b>, Dawei Qi, Yu Han<br />
        Information and Business Intelligence (IBI), 2011<br />
                            [<a href="https://doi.org/10.1007/978-3-642-29084-8_95" target="_blank">Paper</a>] [<a href="doc/IBI2011.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Fiberboard has long been a significant practical material. Computed
tomography (CT) shows great potential for nondestructive testing of the
property and internal structure of fiberboard. In the paper, utilize CT
technology in nondestructive testing procedure of fiberboard as well as
compute the CT number range of different fiberboard tomography slices in
statistic method. Therefore it provides an automatic manipulative procedure in
the fiberboard nondestructive testing based on CT. The fitting linear formula
between CT number and fiberboard density was calculated, because of the
linear relationship exists between fiberboard density and CT number. Thus, a
new method in the nondestructive testing of fiberboard defect and fiberboard
density is provided.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->     
    </table>




        <table border="0" cellspacing="4" cellpadding="2">
             
        </table>


        <h2 id="talks">Peer Review Services</h2>
        Reviewer for: 
    <ul>
    <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">IEEE Transactions on Pattern Analysis and Machine Intelligence</a></li>
    <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target="_blank">IEEE Transactions on Neural Networks and Learning Systems</a></li>
    <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639" target="_blank">IEEE Access</a></li>    
    <li><a href="https://ees.elsevier.com/pr/" target="_blank">Pattern Recognition</a> </li>
    <li><a href="https://dl.acm.org/journal/tkdd" target="_blank">ACM Transactions on Knowledge Discovery from Data</a></li>
    <li><a href="https://onlinelibrary.wiley.com/journal/14678640" target="_blank">Computational Intelligence</a></li>   
    <li><a href="https://www.springer.com/journal/12652/" target="_blank">Journal of Ambient Intelligence and Humanized Computing</a></li>  
    <li><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging" target="_blank">Journal of Medical Imaging</a></li>    
    <li><a href="https://www.journals.elsevier.com/international-journal-of-medical-informatics" target="_blank">International Journal of Medical Informatics</a></li>
    </ul>


        <h2 id="teaching">Teaching Assistant</h2>

        <ul>
               
            <li>BME/ECE 6885: Computer Vision (Fall 2020, Spring  2018)</li>
            <li>BME 2820: Biomedical Engineering Programming I [MATLAB Programming] (Spring 2020, Fall 2019, Spring 2019)</li>
            <li>BME 2825: Biomedical Engineering Programming II [C Programming] (Fall 2019)</li>
            <li>BME 3915W: Biomedical Engineering Capstone Project Lab I (Spring  2018)</li>
            <li>BME/ECE 6850: Pattern Recognition (Fall 2018, Fall 2017)</li>
            <li>BME/ECE  6840: Digital  Image Processing (Spring  2017)</li>
            <li>CSCI 6212: Design and Analysis of Algorithms (Fall 2016)</li>
            <li>CSCI 3362/6362: Probability for Computer Science (Spring 2016)</li>
        </ul>
    
            <h2 id="volunteer">Volunteer</h2>

        <ul>
               
            <li>IEEE Applied Imagery Pattern Recognition Workshop (2019, 2018, 2017, 2016)</li>
            <li>IEEE International Symposium on Biomedical Imaging Conference (2018)</li>

        </ul>
    
<!--

        <h2 id="services">Services</h2>


        <ul>
</ul>


        <h2 id="honors">Honors &amp; Awards</h2>


        <table style="border-spacing:2px">
       
        </table>


        <h2 id="miscellany">Miscellany</h2>


 -->         

        <div id="footer">
                <div id="footer-text">
                </div>
            <p align="right">Last Updated by Shuyue Guan: <span id="demo"></span>.</p>

            <script>
            document.getElementById("demo").innerHTML = document.lastModified;
            </script>
        </div>
 
	    <script src="js/abstract.js"></script>
        <script src="js/full_img.js"></script>
</body>
</html>

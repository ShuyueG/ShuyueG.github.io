<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
    
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-J8W7M6WWKM"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-J8W7M6WWKM');
    </script>
    <meta name="google-site-verification" content="PTV7OnJ3OiGE8EYWc_lFDdvFRMZj62r69k9Nm6MOlok" />
    
        <link rel="shortcut icon" href="imgs/myIcon.jpg" />
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
        <meta name="keywords" content="Shuyue, Shuyue Guan, Guan Shuyue, BME, Machine Learning, Computer Vision, The George Washington University" />
        <meta name="description" content="Shuyue Guan's home page" />
        <link rel="stylesheet" href="./css/jemdoc.css" type="text/css" />
    <link rel="stylesheet" href="./css/full_img.css" type="text/css" />
    <link rel="stylesheet" href="./css/zoom_img.css" type="text/css" />
    <link rel="stylesheet" href="./css/abstract.css" type="text/css" />

        <title>Shuyue Guan</title>

</head>
<body>
	<div class="dropdown" style="float:right;">
	  <button class="dropbtn" ><a href="#volunteer" style="color: inherit;">Volunteer</a></button>
	</div>
	
	<div class="dropdown" style="float:right;">
	  <button class="dropbtn" ><a href="#teaching" style="color: inherit;">TA</a></button>
	</div>
	
	<div class="dropdown" style="float:right;">
	  <button class="dropbtn" ><a href="#talks" style="color: inherit;">Service</a></button>
	</div>

	<div class="dropdown" style="float:right;">
	  <button class="dropbtn" >Presentation</button>
	  <div class="dropdown-content" >
	    <a href="#ivtalks">Invited Talks</a>
	    <a href="#conftalks">Conferences</a>
		<a href="#othertalks">Others</a>
	  </div>
	</div>
	
	<div class="dropdown" style="float:right;">
	  <button class="dropbtn">Publication</button>
	  <div class="dropdown-content" style = "min-width: 260px;">
		<p style="margin: 0; font-size:14px; background-color: #878787; color: white; text-align: center;">Recent Projects</p>
	    <a href="#AIML">Medical AI/ML</a>
	    <a href="#XAI">Transparent Deep Learning</a>
		<a href="#DLApp">Deep Learning Applications on Medical Images</a>
		<p style="margin: 0; font-size:14px; background-color: #878787; color: white; text-align: center;">Past Projects</p>
	    <a href="#Hyperspectral">Hyperspectral Images-based Cardiac Ablation Lesion Detection</a>
		<a href="#FrostPoint">Climate (Frost Point) Data Collection and Analysis by R</a>
		<a href="#Wooden">Non-destructive Testing for Wooden Materials</a>
	  </div>
	</div>


	<br />
        <div id="layout-content" style="margin-top:25px">
                <table cellspacing="0">
                        <tbody>
                                <tr>
                                        <td width="1000">
                                                
                                            <table width="100%" cellspacing="0">
          <tbody>
                                <tr>
                                        <td width="25%" height="75"  id="titlename"><div id="toptitle"><h1>Shuyue Guan </h1>
                                  </div></td>
                                        <td id="hide"  width="54%"><img src="imgs/name.png" height="46" style="margin-bottom:-12px" /></td>
                                </tr>
    </tbody>
          </table>
                                          

                                         <p style="text-align:justify;">Visiting Scientist, <a href="https://www.fda.gov/about-fda/cdrh-offices/division-imaging-diagnostics-and-software-reliability" target="_blank">DIDSR</a>/OSEL/CDRH/FDA</p>
                                        <p style="text-align:justify;">Ph.D. received from the George Washington University (GWU), advised by <a href="https://engineering.gwu.edu/murray-loew" target="_blank">Murray H. Loew</a>. Primary research interests: image processing, machine learning, deep learning, and their applications in medical imaging. Recent researches: in silico medical AI, image segmentation synthesis, data separability measure, and learnability for deep learning models (transparent deep learning). Previous studies: deep learning applications on medical images (BME program at GWU), hyperspectral images-based cardiac ablation lesion detection (BME program at GWU), and non-destructive testing of wood composite panel internal defect (Biophysics program at Northeast Forestry University, China).</p></td>

                                        <td width="60">
                                        </td>

                                        <td><center>
                                          <img src="imgs/bio-photo_doc.jpg" border="0" style="margin-top:0px" width="180" /><br />
                                        </center>
                                          <!-- <center>This pencil sketch is generated </br> by this <a href="http://www.cse.cuhk.edu.hk/leojia/projects/pencilsketch/pencil_drawing.htm">algorithm</a>.</center> --></td>
</tr>
                        </tbody>
                </table>


                
            <a href="doc/Academic_CV_PUB_EN.pdf">Curriculum Vitae</a><br />
            
                <br />Email: frank.shuyue.guan AT gmail DOT com
                <table width="100%" cellspacing="0">
                  <tbody>
                    <tr>
                      <td>&nbsp;</td>
                    </tr>
                  </tbody>
                </table>
<table width="100%" cellspacing="0">
          <tbody>
                                <tr>
                                        <td width="5%">
                                                <center>
                                                        <a href="https://scholar.google.com/citations?user=F0ABc9cAAAAJ&hl=en"><img src="imgs/gscholar_icon.png" title="Google Scholar" border="0" style="margin-top:0px" height="43" width="43" /></a><br />
                                                </center>
                                        </td><!-- <td width="60" /> -->


                                        <td width="5%">
                                                <center>
                                                  <a href="https://www.researchgate.net/profile/Shuyue_Guan" target="_blank"><img src="imgs/researchgate_icon.png" title="ResearchGate" border="0" style="margin-top:0px" height="43" width="43" /></a><br />
                                                </center>
                                        </td>
 										<td width="5%">
                                                <center>
                                                  <a href="https://www.webofscience.com/wos/author/record/GQQ-3858-2022" target="_blank"><img src="imgs/webofscience_icon.png" title="Clarivate: Web of Science" border="0" style="margin-top:0px" height="43" width="43" /></a><br />
                                                </center>
                                        </td>
                                        <td width="5%">
                                                <center>
                                                  <a href="https://github.com/ShuyueG" target="_blank"><img src="imgs/github_icon.png" title="GitHub" border="0" style="margin-top:0px" height="43" width="43" /></a><br />
                                                </center>
                                        </td>

                                        <td width="5%">
                                            <center>
                                            <a href="https://www.linkedin.com/in/shuyueg/"><img src="imgs/linkedin_icon.png" title="LinkedIn" border="0" style="margin-top:0px" height="43" width="43" /></a><br />
                                          </center>
                                        </td>
                                        <td width="5%">&nbsp;</td>

                                        <td width="70%" align="right"><br />
                                        <i>"You the wise, tell me, why should our days leave us, never to return?"</i> - Ziqing Zhu</td>
                                </tr>
    </tbody>
          </table>



                <div class="table-responsive">
                        <h2 id="news">
                        </h2>
                </div>


                <table id="navigate">
                        <thead role="rowgroup">
     
                        </thead>
                </table>
        </div>




<h2 id="publication">Recent Projects &amp; Publications</h2>

<div class="areatitle" id="AIML">Medical AI/ML</div>
	<table width="961" border="0" cellpadding="2" cellspacing="10">
		
		
				<!-- paper strat -->
				                <tr>
				                        <td width="180" valign="center">
				                                <center>
				
				                                    <!-- Zoom in -->
				                                    <div class="img-hover-zoom">
				                                    <img class="myImg" src="imgs/paper_ssiai2024.png" alt="Synthetic segmentation contours and the convergence to their seed polygon shape." width="180px" />
				                                        </div>
				                                    
				                                    <!-- The full-size Modal -->
				                                    <div id="myModal" class="modal">
				                                      <span class="close">&times;</span>
				                                      <img class="modal-content" id="img01">
				                                      <div id="caption"></div>
				                                    </div>
				
				                                </center>
				                        </td>
				
				                        <td width="714" valign="top">
				
				<papertitle>Restorable synthesis: average synthetic segmentation converges to a polygon approximation of an object contour in medical images</papertitle><br />
				        
				        <b>Shuyue Guan</b>, Ravi K. Samala, Seyed M. M. Kahaki, Weijie Chen<br />
				        <i>Accepted by</i> IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI), 2024<br />
				                           [<a href="https://doi.org/10.1109/SSIAI59505.2024.10508669" target="_blank">Paper</a>] [<a href="doc/SSIAI2024.pdf" target="_blank">PDF</a>]
				
				<button class="collapsible">Abstract</button>
				<div class="content">
				  <p>
				Synthesis of segmentation contours is useful in evaluating truthing methods, <i>i.e.</i>, the establishment of a segmentation reference standard by combining multiple segmentation results (<i>e.g.</i>, by multiple experts). In contrast to a real-world application where the ground truth is often not available, the ground truth of objects is defined in synthetic data. Contours with combinations of segmentation errors, as compared to the defined ground truth, can be synthesized. <b>A desired property of segmentation contour synthesis for evaluating truthing methods, which we call the restorability property, is that the average of multiple segmentation contours can converge to the truth contour.</b> This property is desired because such a dataset can serve as a benchmark for evaluating if commonly used truthing methods have bias. We developed a segmentation contour synthesis tool that has the restorability property and conducted simulation studies to validate this tool.
				</p>
				        
				</div>
				                        </td>
				                </tr>
				          <!-- paper end -->
		
		<!-- paper strat -->
				                <tr>
				                        <td width="180" valign="center">
				                                <center>
				
				                                    <!-- Zoom in -->
				                                    <div class="img-hover-zoom">
				                                    <img class="myImg" src="imgs/stochastic_digital_human_2023.png" alt="Classification of methods to generate digital humans for in silico clinical trials." width="180px" />
				                                        </div>
				                                    
				                                    <!-- The full-size Modal -->
				                                    <div id="myModal" class="modal">
				                                      <span class="close">&times;</span>
				                                      <img class="modal-content" id="img01">
				                                      <div id="caption"></div>
				                                    </div>
				
				                                </center>
				                        </td>
				
				                        <td width="714" valign="top">
				
				<papertitle>The stochastic digital human is now enrolling for in silico imaging trials – Methods and tools for generating digital cohorts</papertitle><br />
				        
				        Aldo Badano, MIguel Lago, Elena Sizikova, Jana Delfino, <b>Shuyue Guan</b>, Mark A Anastasio, <br />
						Berkman Sahiner<br />
				        Progress in Biomedical Engineering, 2023<br />
				                            [<a href="https://doi.org/10.1088/2516-1091/ad04c0" target="_blank">Paper</a>]
				
				<button class="collapsible">Abstract</button>
				<div class="content">
				  <p>
			    Randomized clinical trials, while often viewed as the highest evidentiary bar by which to judge the quality of a medical intervention, are far from perfect. In silico imaging trials are computational studies that seek to ascertain the performance of a medical device by collecting this information entirely via computer simulations. The benefits of in silico trials for evaluating new technology include significant resource and time savings, minimization of subject risk, the ability to study devices that are not achievable in the physical world, allow for the rapid and effective investigation of new technologies and ensure representation from all relevant subgroups. To conduct in silico trials, digital representations of humans are needed. We review the latest developments in methods and tools for obtaining digital humans for in silico imaging studies. First, we introduce terminology and a classification of digital human models. Second, we survey available methodologies for generating digital humans with healthy status and for generating diseased cases and discuss briefly the role of augmentation methods. Finally, we discuss approaches for sampling digital cohorts and understanding the trade-offs and potential for study bias associated with selecting specific patient distributions.
				</div>
				                        </td>
				                </tr>
				          <!-- paper end -->
						  
		<!-- paper strat -->
		                <tr>
		                        <td width="180" valign="center">
		                                <center>
		
		                                    <!-- Zoom in -->
		                                    <div class="img-hover-zoom">
		                                    <img class="myImg" src="imgs/paper_spie2023.png" alt=" GUI of the MISS-tool for generating synthetic segmentations. The main window (upper-left) provides the
											segmentation evaluation results of 24 metric to quantify the accuracy of synthetic segmentation. The truth mask is shown
											in the blue area, and the green contour shows the synthesized segmentation." width="180px" />
		                                        </div>
		                                    
		                                    <!-- The full-size Modal -->
		                                    <div id="myModal" class="modal">
		                                      <span class="close">&times;</span>
		                                      <img class="modal-content" id="img01">
		                                      <div id="caption"></div>
		                                    </div>
		
		                                </center>
		                        </td>
		
		                        <td width="714" valign="top">
		
		<papertitle>MISS-tool: medical image segmentation synthesis tool to emulate segmentation errors</papertitle><br />
		        
		        <b>Shuyue Guan</b>, Ravi K Samala, Arian Arab, Weijie Chen<br />
		        SPIE Medical Imaging: Computer-Aided Diagnosis, 2023<br />
		                            [<a href="https://doi.org/10.1117/12.2653650" target="_blank">Paper</a>] [<a href="doc/SPIE2023.pdf" target="_blank">PDF</a>]
		
		<button class="collapsible">Abstract</button>
		<div class="content">
		  <p>
Segmentation of medical images with known ground truth is useful for investigating properties of performance metrics and comparing different approaches of combining multiple manual segmentations to establish a reference standard, thereby informing selection of performance metrics and truthing methods. For medical images, however, segmentation ground truth is typically not available. One way of synthesizing segmentation errors is to use regular geometric objects as ground truth, but they lack the complexity and variability of real anatomical objects. To address this problem, we developed a medical image segmentation synthesis (MISS)-tool. The MISS-tool emulates segmentations by adjusting truth masks of anatomical objects extracted from real medical images. We categorized six types of segmentation errors and developed contour transformation tools with a set of user-adjustable parameters to modify the defined truth contours to emulate different types of segmentation errors, thereby generating synthetic segmentations. In a simulation study, we synthesized multiple segmentations to emulate algorithms or observers with pre-defined sets of segmentation errors (e.g., under/over-segmentation) using 220 lung nodule cases from the LIDC lung computed tomography dataset. We verified that the synthetic segmentation results manifest the type of errors that are consistent with our pre-configured setting. Our tool is useful for synthesizing a range of segmentation errors within a clinical segmentation task.</p>
		        
		</div>
		                        </td>
		                </tr>
		          <!-- paper end -->
		
	<!-- paper strat -->
	                <tr>
	                        <td width="180" valign="center">
	                                <center>
	
	                                    <!-- Zoom in -->
	                                    <div class="img-hover-zoom">
	                                    <img class="myImg" src="imgs/paper_Arian2023_tmp.png" alt="tmp img" width="180px" />
	                                        </div>
	                                    
	                                    <!-- The full-size Modal -->
	                                    <div id="myModal" class="modal">
	                                      <span class="close">&times;</span>
	                                      <img class="modal-content" id="img01">
	                                      <div id="caption"></div>
	                                    </div>
	
	                                </center>
	                        </td>
	
	                        <td width="714" valign="top">
	
	<papertitle>Effect of color-normalization on deep learning segmentation models for tumor-infiltrating lymphocytes scoring using breast cancer histopathology images</papertitle><br />
	        
	        Arian Arab, Victor Garcia, <b>Shuyue Guan</b>, Brandon D Gallas, Berkman Sahiner, Nicholas Petrick, Weijie Chen<br />
	        SPIE Medical Imaging: Digital and Computational Pathology, 2023<br />
	                            [<a href="https://doi.org/10.1117/12.2653989" target="_blank">Paper</a>] [<a href="doc/Arab_SPIE2023.pdf" target="_blank">PDF</a>]
	
	<button class="collapsible">Abstract</button>
	<div class="content">
	  <p>Studies have shown that the increased presence of tumor-infiltrating lymphocytes (TILs) is associated with better long-term clinical outcomes and survival, which makes TILs a potentially useful quantitative biomarker. In clinics, pathologists’ visual assessment of TILs in biopsies and surgical resections result in a quantitative score (TILs-score). The Tumor-infiltrating lymphocytes in breast cancer (TiGER) challenge is the first public challenge on automated TILs-scoring algorithms using whole slide images of hematoxylin and eosin-stained (H&E) slides of human epidermal growth factor receptor-2 positive (HER2+) and triple-negative breast cancer (TNBC) patients. We participated in the TiGER challenge and developed algorithms for tumor-stroma segmentation, TILs cell detection, and TILs-scoring. The whole slide images in this challenge are from three sources, each with apparent color variations. We hypothesized that color-normalization may improve the cross-source generalizability of our deep learning models. Here, we expand our initial work by implementing a color-normalization technique and investigate its effect on the performance of our segmentation model. We compare the segmentation performance before and after color-normalization by cross validating the models on the three datasets. Our results show a substantial increase in the performance of the segmentation model after color-normalization when trained and tested on different sources. This might potentially improve the model’s generalizability and robustness when applied to the external sequestered test set from the TiGER challenge.</p>
	        
	</div>
	                        </td>
	                </tr>
	          <!-- paper end -->
	
	                    <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                    <img class="myImg" src="imgs/paper_aipr2022.png" alt="Six synthetic segmentations overlapped by according real segmentations. Yellow: True Positive; light blue: False Positive; green: False Negative; dark blue: True Negative." width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Informing selection of performance metrics for medical image segmentation evaluation using configurable synthetic errors</papertitle><br />
        
        <b>Shuyue Guan</b>, Ravi K. Samala, Weijie Chen<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2022<br />
                            [<a href="http://dx.doi.org/10.1109/AIPR57179.2022.10092203" target="_blank">Paper</a>] [<a href="http://arxiv.org/abs/2212.14828" target="_blank">Arxiv</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Machine learning-based segmentation in medical imaging is widely used in clinical applications from diagnostics to radiotherapy treatment planning. Segmented medical images with ground truth are useful for investigating the properties of different segmentation performance metrics to inform metric selection. Regular geometrical shapes are often used to synthesize segmentation errors and illustrate properties of performance metrics, but they lack the complexity of anatomical variations in real images. In this study, we present a tool to emulate segmentations by adjusting the reference (truth) masks of anatomical objects extracted from real medical images. Our tool is designed to modify the defined truth contours and emulate different types of segmentation errors with a set of user-configurable parameters. We defined the ground truth objects from 230 patient images in the Glioma Image Segmentation for Radiotherapy (GLIS-RT) database. For each object, we used our segmentation synthesis tool to synthesize 10 versions of segmentation (i.e., 10 simulated segmentors or algorithms), where each version has a pre-defined combination of segmentation errors. We then applied 20 performance metrics to evaluate all synthetic segmentations. We demonstrated the properties of these metrics, including their ability to capture specific types of segmentation errors. By analyzing the intrinsic properties of these metrics and categorizing the segmentation errors, we are working toward the goal of developing a decision-tree tool for assisting in the selection of segmentation performance metrics.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
	</table>

<div class="areatitle" id="XAI">Transparent Deep Learning</div>
    
      <table width="961" border="0" cellpadding="2" cellspacing="10">
		  
		  
		             <!-- paper strat -->
		                  <tr>
		                          <td width="180" valign="center">
		                                  <center>
		  
		                                      <!-- Zoom in -->
		                                      <div class="img-hover-zoom">
		                                          <img class="myImg" src="imgs/paper_NNest.png" alt="Fitting curve of 1/b=f(N,L) in 2-D" width="180px" />
		                                          </div>
		                                      
		                                      <!-- The full-size Modal -->
		                                      <div id="myModal" class="modal">
		                                        <span class="close">&times;</span>
		                                        <img class="modal-content" id="img01">
		                                        <div id="caption"></div>
		                                      </div>
		  
		                                  </center>
		                          </td>
		  
		                          <td width="714" valign="top">
		  
		  <papertitle>The training accuracy of two-layer neural networks: its estimation and understanding using random datasets</papertitle><br />
		          
		          <b>Shuyue Guan</b>, Murray Loew<br />
		          IEEE Applied Imagery Pattern Recognition (AIPR), 2023<br />
		                              [<a href="https://doi.org/10.1109/AIPR60534.2023.10440662" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2010.13380" target="_blank">Arxiv</a>]
		  
		  <button class="collapsible">Abstract</button>
		  <div class="content">
		    <p>
			Although the neural network (NN) technique plays a vital role in machine learning, understanding the mechanism of NN models and the transparency of deep learning still require more basic research. In this study, we propose a novel theory based on space partitioning to estimate the approximate training accuracy for two-layer neural networks on random datasets without training. There appear to be no other studies that have proposed a method to estimate training accuracy without using input data and/or trained models. <b> Our method estimates the training accuracy for two-layer fully-connected neural networks on two-class random datasets using only three arguments: the dimensionality of inputs (d), the number of inputs (N), and the number of neurons in the hidden layer (L). </b>We have verified our method using real training accuracies in our experiments. The results indicate that the method will work for any dimension, and the proposed theory could also extend to estimate deeper NN models. The main purpose of this paper is to understand the mechanism of NN models by the approach of estimating training accuracy but not to analyze their generalization nor their performance in real-world applications. This study may provide a starting point for a new way for researchers to make progress on the difficult problem of understanding deep learning.
			</p>
		          
		  </div>
		                          </td>
		                  </tr>
		            <!-- paper end -->
		  
		  <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_ictai2020.png" alt="Two clusters (classes) dataset with different label assignments" width="180px" />
                                        </div>      
                                                                        
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">
[J] <papertitle>A Distance-based Separability Measure for Internal Cluster Validation</papertitle><br />
                            <b>Shuyue Guan</b>, Murray Loew<br />
        International Journal on Artificial Intelligence Tools, 2022<br />
                            [<a href="https://doi.org/10.1142/S0218213022600053" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2106.09794" target="_blank">Arxiv</a>] [<a href="https://github.com/ShuyueG/CVI_using_DSI" target="_blank">Code</a>]<br />
                            <p></p>
[C] <papertitle>An Internal Cluster Validity Index Using a Distance-based Separability Measure</papertitle><br />
        
        International Conference on Tools with Artificial Intelligence (ICTAI), 2020<br />
                            /Long Paper & Oral Presentation/ /Peer-review, Acceptance Rate: 26%/<br />
                            [<a href="https://doi.org/10.1109/ICTAI50040.2020.00131" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2009.01328" target="_blank">Arxiv</a>] [<a href="https://youtu.be/TfihU4uBA_8" target="_blank">Video</a>] [<a href="https://github.com/ShuyueG/CVI_using_DSI" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>To evaluate clustering results is a significant part of cluster analysis. There are no true class labels for clustering in typical unsupervised learning. Thus, a number of internal evaluations, which use predicted labels and data, have been created. They are also named internal cluster validity indices (CVIs). Without true labels, to design an effective CVI is not simple because it is similar to create a clustering method. And, to have more CVIs is crucial because there is no universal CVI that can be used to measure all datasets, and no specific method for selecting a proper CVI for clusters without true labels. Therefore, to apply more CVIs to evaluate clustering results is necessary. In this paper, <b>we propose a novel CVI - called Distance-based Separability Index (DSI), based on a data separability measure</b>. We applied the DSI and eight other internal CVIs including early studies from Dunn (1974) to most recent studies CVDD (2019) as comparison. We used an external CVI as ground truth for clustering results of five clustering algorithms on 12 real and 97 synthetic datasets. Results show DSI is an effective, unique, and competitive CVI to other compared CVIs. In addition, we summarized the general process to evaluate CVIs and created a new method - rank difference - to compare the results of CVIs.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->

          
                  <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_dsi.png" alt="An example of two-class dataset in 2-D shows the definition and computation of the DSI." width="180px" />
                                        </div>      
                                                                        
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>A Novel Intrinsic Measure of Data Separability</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        Applied Intelligence, 2022<br />
                          [<a href="https://doi.org/10.1007/s10489-022-03395-6" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2109.05180" target="_blank">Arxiv</a>] [<a href="https://github.com/ShuyueG/GAN_evaluation_LS" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>In machine learning, the performance of a classifier depends on both the classifier model and the separability/complexity of datasets. To quantitatively measure the separability of datasets, in this study, we propose an <em>intrinsic measure</em> – the Distance-based Separability Index (DSI), which is independent of the classifier model. We then formally show that the <strong>DSI can indicate whether the distributions of datasets are identical for any dimensionality</strong>. DSI can measure separability of datasets because we consider the situation in which different classes of data are mixed in the same distribution to be the most difficult for classifiers to separate. And, DSI is verified to be an effective separability measure by comparing it to state-of-the-art separability/complexity measures using synthetic datasets and real datasets (CIFAR-10/100). Having demonstrated the DSI’s ability to compare distributions of samples, our other studies show that it can be used in other separability-based applications, such as measuring the performance of generative adversarial networks (GANs) and evaluating the results of clustering methods.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
                    <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                    <img class="myImg" src="imgs/paper_aipr2021.png" alt="The first row shows one of the abnormal (tumor) ROIs and its truth mask. Other rows show the CAMs of this ROI generated by using trained CNN classifiers and Grad-CAM algorithm." width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2021<br />
                            [<a href="https://doi.org/10.1109/AIPR52630.2021.9762077" target="_blank">Paper</a>] [<a href="http://arxiv.org/abs/2201.02771" target="_blank">Arxiv</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Instead of using current deep-learning segmentation models (like the UNet and variants), we approach the segmentation problem using trained Convolutional Neural Network (CNN) classifiers, which automatically extract important features from classified targets for image classification. Those extracted features can be visualized and formed heatmaps using Gradient-weighted Class Activation Mapping (Grad-CAM). This study tested whether the heatmaps could be used to segment the classified targets. We also proposed an evaluation method for the heatmaps; that is, to re-train the CNN classifier using images filtered by heatmaps and examine its performance. We used the mean-Dice coefficient to evaluate segmentation results. Results from our experiments show that heatmaps can locate and segment partial tumor areas. But only use of the heatmaps from CNN classifiers may not be an optimal approach for segmentation. In addition, we have verified that the predictions of CNN classifiers mainly depend on tumor areas, and dark regions in Grad-CAM’s heatmaps also contribute to classification.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_GANmeasureLS.png" alt="Real and generated datasets from virtual GANs on MNIST" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>A Novel Measure to Evaluate Generative Adversarial Networks Based on Direct Analysis of Generated Images</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        Neural Computing and Applications, 2021<br />
                            [<a href="https://doi.org/10.1007/s00521-021-06031-5" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2002.12345" target="_blank">Arxiv</a>] [<a href="https://github.com/ShuyueG/GAN_evaluation_LS" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>The Generative Adversarial Network (GAN) is a state-of-the-art technique in the field of deep learning. A number of recent papers address the theory and applications of GANs in various fields of image processing. Fewer studies, however, have directly evaluated GAN outputs. Those that have been conducted focused on using classification performance, e.g., Inception Score (IS) and statistical metrics, e.g., Fréchet Inception Distance (FID). Here, we consider a fundamental way to evaluate GANs by directly analyzing the images they generate, instead of using them as inputs to other classifiers. We characterize the performance of a GAN as an image generator according to three aspects: 1) Creativity: non-duplication of the real images. 2) Inheritance: generated images should have the same style, which retains key features of the real images. 3) Diversity: generated images are different from each other. A GAN should not generate a few different images repeatedly. Based on the three aspects of ideal GANs, <b>we have designed the Likeness Score (LS) to evaluate GAN performance</b>, and have applied it to evaluate several typical GANs. We compared our proposed measure with two commonly used GAN evaluation methods: IS and FID, and four additional measures. Furthermore, we discuss how these evaluations could help us deepen our understanding of GANs and improve their performance.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                    <img class="myImg" src="imgs/paper_aipr2020.png" alt="Two subsets of random two-class data set and their
decision boundaries" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Understanding the Ability of Deep Neural Networks to Count Connected Components in Images</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2020<br />
                            [<a href="https://doi.org/10.1109/AIPR50011.2020.9425331" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2101.01386" target="_blank">Arxiv</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Humans can count very fast by subitizing, but slow substantially as the number of objects increases. Previous studies have shown a trained deep neural network (DNN) detector can count the number of objects in an amount of time that increases slowly with the number of objects. Such a phenomenon suggests the subitizing ability of DNNs, and unlike humans, it works equally well for large numbers. Many existing studies have successfully applied DNNs to object counting, but few studies have studied the subitizing ability of DNNs and its interpretation. In this paper, <b>we found DNNs do not have the ability to generally count connected components</b>. We provided experiments to support our conclusions and explanations to understand the results and phenomena of these experiments. <b>We proposed three ML-learnable characteristics to verify learnable problems for ML models, such as DNNs, and explain why DNNs work for specific counting problems but cannot generally count connected components</b>.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
       
          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_icmla2020.png" alt="Adversarial examples generated by pairs of data" width="180px" />
                                        </div>      
                                                                        
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Analysis of Generalizability of Deep Neural Networks Based on the Complexity of Decision Boundary</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        International Conference on Machine Learning and Applications (ICMLA), 2020<br />
                            /Full Paper & Oral Presentation/ /Double-blind Peer-review, Acceptance Rate: 25%/<br />
                            [<a href="https://doi.org/10.1109/ICMLA51294.2020.00025" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2009.07974" target="_blank">Arxiv</a>] [<a href="https://youtu.be/mJbmPiuGTcU" target="_blank">Video</a>] [<a href="https://github.com/ShuyueG/decision-boundary-complexity-score" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>For supervised learning models, the analysis of generalization ability (generalizability) is vital because the generalizability expresses how well a model will perform on unseen data. Traditional generalization methods, such as the VC dimension, do not apply to deep neural network (DNN) models. Thus, new theories to explain the generalizability of DNNs are required. In this study, we hypothesize that the DNN with a simpler decision boundary has better generalizability by the law of parsimony (Occam's Razor). <b>We create the decision boundary complexity (DBC) score to define and measure the complexity of decision boundary of DNNs.</b> The idea of the DBC score is to generate data points (called adversarial examples) on or near the decision boundary. Our new approach then measures the complexity of the boundary using the entropy of eigenvalues of these data. The method works equally well for high-dimensional data. We use training data and the trained model to compute the DBC score. And, the ground truth for model's generalizability is its test accuracy. Experiments based on the DBC score have verified our hypothesis. The DBC is shown to provide an effective method to measure the complexity of a decision boundary and gives a quantitative measure of the generalizability of DNNs.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->


          
 
          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_GANmeasure.png" alt="Problems of generated images from the perspective of distribution" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Evaluation of Generative Adversarial Network Performance Based on Direct Analysis of Generated Images</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2019<br />
                            [<a href="https://doi.org/10.1109/AIPR47015.2019.9174595" target="_blank">Paper</a>] [<a href="doc/AIPR2019.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Recently, a number of papers have addressed the theory and applications of the Generative Adversarial Network (GAN) in various fields of image processing. Fewer studies, however, have directly evaluated GAN outputs. Those that have been conducted focused on using classification performance and statistical metrics. In this paper, we consider a fundamental way to evaluate GANs by directly analyzing the images they generate, instead of using them as inputs to other classifiers. <b>We consider an ideal GAN according to three aspects</b>: 1) Creativity: non-duplication of the real images. 2) Inheritance: generated images should have the same style, which retains key features of the real images. 3) Diversity: generated images are different from each other. <b>Based on the three aspects, we have designed the Creativity-Inheritance-Diversity (CID) index to evaluate GAN performance</b>. We compared our proposed measures with three commonly used GAN evaluation methods: Inception Score (IS), Fréchet Inception Distance (FID) and 1-Nearest Neighbor classifier (1NNC). In addition, we discuss how the evaluation could help us deepen our understanding of GANs and improve their performance.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
          
    </table>

<div class="areatitle" id="DLApp">Deep Learning Applications on Medical Images</div>
    
          <table width="961" border="0" cellpadding="2" cellspacing="10">
			  
			                
                              <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_CFPNetM.png" alt="The CFP module" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>CFPNet-M: A Light-Weight Encoder-Decoder Based Network for Multimodal Biomedical Image Real-Time Segmentation</papertitle><br />
        
        Ange Lou, <b>Shuyue Guan</b>, Murray Loew<br />
        Computers in Biology and Medicine, 2023<br />
                           [<a href="https://doi.org/10.1016/j.compbiomed.2023.106579" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2105.04075" target="_blank">Arxiv</a>] [<a href="https://github.com/ShuyueG/CFPNet-Medicine" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Deep learning techniques are proving instrumental in identifying, classifying, and quantifying patterns in medical images. Segmentation is one of the important applications in medical image analysis. The U-Net has become the predominant deep-learning approach to medical image segmentation tasks. Existing U-Net based models have limitations in several respects, however, including: the requirement for millions of parameters in the U-Net, which consumes considerable computational resources and memory; the lack of global information; and incomplete segmentation in difficult cases. To remove some of those limitations, we built on our previous work and applied two modifications to improve the U-Net model: 1) we designed and added the dilated channel-wise CNN module and 2) we simplified the U-shape network. We then proposed a novel light-weight architecture, the Channel-wise Feature Pyramid Network for Medicine (CFPNet-M). To evaluate our method, we selected five datasets from different imaging modalities: thermography, electron microscopy, endoscopy, dermoscopy, and digital retinal images. We compared its performance with several models having a variety of complexities. We used the Tanimoto similarity instead of the Jaccard index for gray-level image comparisons. The CFPNet-M achieves segmentation results on all five medical datasets that are comparable to existing methods, yet require only 8.8 MB memory, and just 0.65 million parameters, which is about 2% of U-Net. Unlike other deep-learning segmentation methods, this new approach is suitable for real-time application: its inference speed can reach 80 frames per second when implemented on a single RTX 2070Ti GPU with an input image size of 256 × 192 pixels.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->

              
                                           <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_CaraNet.png" alt="Overview of the CaraNet." width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

[J] <papertitle>CaraNet: Context Axial Reverse Attention Network for Segmentation of Small Medical Objects</papertitle><br />
        
        Ange Lou, <b>Shuyue Guan</b>, Murray Loew<br />
		Journal of Medical Imaging (JMI), 2023<br />
		                    [<a href="http://dx.doi.org/10.1117/1.JMI.10.1.014005" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2301.13366" target="_blank">Arxiv</a>] [<a href="https://github.com/AngeLouCN/CaraNet" target="_blank">Code</a>] <br />
		                    <p></p>
							
        [C] Ange Lou, <b>Shuyue Guan</b>, Hanseok Ko, Murray Loew<br />
		SPIE Medical Imaging, 2022<br />
                           [<a href="https://doi.org/10.1117/12.2611802" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2108.07368" target="_blank">Arxiv</a>] [<a href="https://github.com/AngeLouCN/CaraNet" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Segmenting medical images accurately and reliably is important for disease diagnosis and treatment. It is a challenging task because of the wide variety of objects’ sizes, shapes, and scanning modalities. Recently, many convolutional neural networks (CNN) have been designed for segmentation tasks and achieved great success. Few studies, however, have fully considered the sizes of objects, and thus most demonstrate poor performance for small objects segmentation. This can have a significant impact on the early detection of diseases. This paper proposes a Context Axial Reserve Attention Network (CaraNet) to improve the segmentation performance on small objects compared with several recent state-of-the-art models. We test our CaraNet on brain tumor (BraTS 2018) and polyp (Kvasir-SEG, CVC-ColonDB, CVC-ClinicDB, CVC-300, and ETIS-LaribPolypDB) segmentation datasets. Our CaraNet achieves the top-rank mean Dice segmentation accuracy, and results show a distinct advantage of CaraNet in the segmentation of small medical objects.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
			  

                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/embc2021.png" alt="(a) Fundus image example, diagnosed with glaucoma, and its Grad-CAM result (b). It highlights the affected area of glaucoma." width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Portable and Affordable Ophthalmic Disease Detection System</papertitle><br />
        
        Teah Serani, Christina Kang, George Saab, <strong>Shuyue
Guan</strong>, Nathan H. Choe, Murray Loew<br />
        International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2021<br />
                            (<a href="https://embs.papercept.net/conferences/conferences/EMBC21/program/EMBC21_ContentListWeb_4.html#thdt1_16"  target="_blank">Paper ThDT1.16</a>) <br />
                           [<a href="doc/EMBC21_2171_FI.pdf">PDF</a>] [<a href="https://drive.google.com/file/d/1diMa0YKu7tZLsWDZsfVYiQaf9uj23Czk/view?usp=sharing" target="_blank">Video</a>] [<a href="https://github.com/A-EYE-George-Washington-University/Model" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>This study introduces an ophthalmic disease detection system that allows users to take a fundus image and detect common eye diseases using a smartphone. The detection is based on a convolutional neural network to classify the various retinal diseases by fundus images. The overall accuracy was 74%, and AUC was 0.93. Grad-CAM was generated to provide heatmaps with visual explanations of the prediction. <strong>Clinical Relevance — The results help promote worldwide eye
health, helping clinicians diagnose retinal diseases with confusing features more easily.</strong></p>
</div>
                        </td>
                </tr>
          <!-- paper end -->



              
                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_dcunet.png" alt="The DC-Block" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>DC-UNet: Rethinking the U-Net Architecture with Dual Channel Efficient CNN for Medical Images Segmentation</papertitle><br />
        
        Ange Lou, <b>Shuyue Guan</b>, Murray Loew<br />
        SPIE Medical Imaging, 2021<br />
                           [<a href="https://doi.org/10.1117/12.2582338" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2006.00414" target="_blank">Arxiv</a>] [<a href="https://github.com/ShuyueG/DC-UNet" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p> Recently, deep learning has become much more popular in computer vision area. The Convolution Neural Network (CNN) has brought a breakthrough in images segmentation areas, especially, for medical images. In this regard, U-Net is the predominant approach to medical image segmentation task. The U-Net not only performs well in segmenting multimodal medical images generally, but also in some tough cases of them. However, we found that the classical U-Net architecture has limitation in several aspects. Therefore, we applied modifications: 1) designed efficient CNN architecture to replace encoder and decoder, 2) applied residual module to replace skip connection between encoder and decoder to improve based on the-state-of-the-art U-Net model. Following these modifications, we designed a novel architecture by adding Dual-Channel blocks in the U-Net model, called Dual Channel U-Net (DC-UNet), as a potential successor to the U-Net architecture. We created a new effective CNN architecture and build the DC-UNet based on this CNN. We have evaluated our model on three datasets with tough cases and have obtained a relative improvement in performance of 2.90%, 1.49% and 11.42% respectively compared with classical U-Net, especially, DC-UNet has about 30% parameters of the U-Net. In addition, we introduced the Tanimoto similarity and used it for gray-to-gray image comparisons instead of the Jaccard similarity.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
              
                        <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_aipr2019_ange.png" alt="Segmentation results of Volunteer #7. (a) Cropped Image (b) Manual ground-truth (c) C-DCNN (d) U-Net (e) MultiResUnet" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Segmentation of Infrared Breast Images Using MultiResUnet Neural Networks</papertitle><br />
        
        Ange Lou, <b>Shuyue Guan</b>, Nada Kamona, Murray Loew<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2019<br />
                            [<a href="https://doi.org/10.1109/AIPR47015.2019.9316541" target="_blank">Paper</a>] [<a href="https://arxiv.org/abs/2011.00376" target="_blank">Arxiv</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Breast cancer is the second leading cause of death for women in the U.S. Early detection of breast cancer is key to higher survival rates to breast cancer patients. We are investigating infrared (IR) thermography as a noninvasive adjunct to mammography for breast cancer screening. IR imaging is radiation-free, pain-free, and non-contact. Automatic segmentation of the breast area from the acquired full-size breast IR images will help limit the area for tumor search, as well as reduce the time and effort costs of manual hand segmentation. Autoencoder-like convolutional and deconvolutional neural networks (C-DCNN) had been applied to automatically segment the breast area in IR images in previous studies. In this study, we applied a state-of-the-art deep-learning segmentation model, MultiResUnet, which consists of an encoder part to capture features and a decoder part for precise localization. It was used to segment the breast area by using a set of breast IR images, collected in our clinical trials by imaging breast cancer patients and normal volunteers with our infrared camera (N2 Imager). The database we used has 450 images, acquired from 14 patients and 16 volunteers. We used a thresholding method to remove interference in the raw images and remapped them from the original 16-bit to 8-bit, and then cropped and segmented the 8-bit images manually. Experiments using leave-one-out cross-validation (LOOCV) and comparison with the ground-truth images by using Tanimoto similarity show that the average accuracy of MultiResUnet is 91.47%, which is about 2% higher than that of the autoencoder. MultiResUnet offers a better approach to segment breast IR images than our previous model.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->

              
                                    <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_spie2019.png" alt="The flowchart of our experiment plan" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Using generative adversarial networks and transfer learning for breast cancer detection by convolutional neural networks</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        SPIE Medical Imaging, 2019<br />
                            [<a href="https://doi.org/10.1117/12.2512671" target="_blank">Paper</a>] [<a href="doc/SPIE2019.pdf">PDF</a>] [<a href="https://github.com/ShuyueG/gan-for-breast-cancer-detection" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>In the U.S., breast cancer is diagnosed in about 12% of women during their lifetime and it is the second leading reason for women’s death. Since early diagnosis could improve treatment outcomes and longer survival times for breast cancer patients, it is significant to develop breast cancer detection techniques. The Convolutional Neural Network (CNN) can extract features from images automatically and then perform classification. To train the CNN from scratch, however, requires a large number of labeled images, which is infeasible for some kinds of medical image data such as mammographic tumor images. In this paper, we proposed two solutions to the lack of training images. 1)To generate synthetic mammographic images for training by the Generative Adversarial Network (GAN). Adding GAN generated images made to train CNN from scratch successful and adding more GAN images improved CNN’s validation accuracy to at most (best) 98.85%. 2)To apply transfer learning in CNN. We used the pre-trained VGG-16 model to extract features from input mammograms and used these features to train a Neural Network (NN)-classifier. The stable average validation accuracy converged at about 91.48% for classifying abnormal vs. normal cases in the DDSM database. Then, we combined the two deep-learning based technologies together. That is to apply GAN for image augmentation and transfer learning in CNN for breast cancer detection. To the training set including real and GAN augmented images, although transfer learning model did not perform better than the CNN, the speed of training transfer learning model was about 10 times faster than CNN training. Adding GAN images can help training avoid over-fitting and image augmentation by GAN is necessary to train CNN classifiers from scratch. On the other hand, transfer learning is necessary to be applied for training on pure real images. To apply GAN to augment training images for training CNN classifier obtained the best classification performance.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                          <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_jmi2019.png" alt="Training accuracy and validation accuracy for training datasets" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>
                                </center>
                        </td>

                        <td width="714" valign="top">

[J] <papertitle>Breast cancer detection using synthetic mammograms from generative adversarial networks in convolutional neural networks</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        Journal of Medical Imaging (JMI), 2019<br />
                            [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6430964/" target="_blank">Paper</a>] [<a href="https://github.com/ShuyueG/gan-for-breast-cancer-detection" target="_blank">Code</a>] <br />
                            <p></p>
[C] International Workshop on Breast Imaging (IWBI), 2018<br />
                            [<a href="https://doi.org/10.1117/12.2318100" target="_blank">Paper</a>] [<a href="https://github.com/ShuyueG/gan-for-breast-cancer-detection" target="_blank">Code</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p> The convolutional neural network (CNN) is a promising technique to detect breast cancer based on mammograms. Training the CNN from scratch, however, requires a large amount of labeled data. Such a requirement usually is infeasible for some kinds of medical image data such as mammographic tumor images. Because improvement of the performance of a CNN classifier requires more training data, the creation of new training images, image augmentation, is one solution to this problem. We applied the generative adversarial network (GAN) to generate synthetic mammographic images from the digital database for screening mammography (DDSM). From the DDSM, we cropped two sets of regions of interest (ROIs) from the images: normal and abnormal (cancer/tumor). Those ROIs were used to train the GAN, and the GAN then generated synthetic images. For comparison with the affine transformation augmentation methods, such as rotation, shifting, scaling, etc., we used six groups of ROIs [three simple groups: affine augmented, GAN synthetic, real (original), and three mixture groups of any two of the three simple groups] for each to train a CNN classifier from scratch. And, we used real ROIs that were not used in training to validate classification outcomes. Our results show that, to classify the normal ROIs and abnormal ROIs from DDSM, adding GAN-generated ROIs in the training data can help the classifier prevent overfitting, and on validation accuracy, the GAN performs about 3.6% better than affine transformations for image augmentation. Therefore, GAN could be an ideal augmentation approach. The images augmented by GAN or affine transformation cannot substitute for real images to train CNN classifiers because the absence of real images in the training set will cause over-fitting.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
              
           <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_aipr2018.png" alt="Segmentation results of one patient" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>
                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Segmentation of Thermal Breast Images Using Convolutional and Deconvolutional Neural Networks</papertitle><br />
        
        <b>Shuyue Guan</b>, Nada Kamona, Murray Loew<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2018<br />
                            [<a href="https://doi.org/10.1109/AIPR.2018.8707379" target="_blank">Paper</a>] [<a href="doc/AIPR2018.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Breast cancer is the second leading cause of death for women in the U.S. Early detection of breast cancer has been shown to be the key to higher survival rates for breast cancer patients. We are investigating infrared thermography as a noninvasive adjunctive to mammography for breast screening. Thermal imaging is safe, radiation-free, pain-free, and non-contact. Segmentation of breast area from the acquired thermal images will help limit the area for tumor search and reduce the time and effort needed for manual hand segmentation. Autoencoder-like convolutional and deconvolutional neural networks (C-DCNN) are promising computational approaches to automatically segment breast areas in thermal images. In this study, we apply the C-DCNN to segment breast areas from our thermal breast images database, which we are collecting in our clinical trials by imaging breast cancer patients with our infrared camera (N2 Imager). For training the C-DCNN, the inputs are 132 gray-value thermal images and the corresponding manually-cropped breast area images (binary masks to designate the breast areas). For testing, we input thermal images to the trained C-DCNN and the output after post-processing are the binary breast-area images. Cross-validation and comparison with the ground-truth images show that the C-DCNN is a promising method to segment breast areas. The results demonstrate the capability of C-DCNN to learn essential features of breast regions and delineate them in thermal images.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
                
           <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>

                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_aipr2017.png" alt="Comparing of the three CNN classification models" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>
                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Breast Cancer Detection Using Transfer Learning in Convolutional Neural Networks</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew<br />
        IEEE Applied Imagery Pattern Recognition (AIPR), 2017<br />
                            [<a href="https://doi.org/10.1109/AIPR.2017.8457948" target="_blank">Paper</a>] [<a href="doc/AIPR2017.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>In the U.S., breast cancer is diagnosed in about 12 % of women during their lifetime and it is the second leading reason for women's death. Since early diagnosis could improve treatment outcomes and longer survival times for breast cancer patients, it is significant to develop breast cancer detection techniques. The Convolutional Neural Network (CNN) can extract features from images automatically and then perform classification. To train the CNN from scratch, however, requires a large number of labeled images, which is infeasible for some kinds of medical image data such as mammographic tumor images. A promising solution is to apply transfer learning in CNN. In this paper, we firstly tested three training methods on the MIAS database: 1) trained a CNN from scratch, 2) applied the pre-trained VGG-16 model to extract features from input mammograms and used these features to train a Neural Network (NN)-classifier, 3) updated the weights in several final layers of the pre-trained VGG-16 model by back-propagation (fine-tuning) to detect abnormal regions. We found that method 2) is ideal for study because the classification accuracy of fine-tuning model was just 0.008 higher than that of feature extraction model but time cost of feature extraction model was only about 5% of that of the fine-tuning model. Then, we used method 2) to classify regions: benign vs. normal, malignant vs. normal and abnormal vs. normal from the DDSM database with 10-fold cross validation. The average validation accuracy converged at about 0.905 for abnormal vs. normal cases, and there was no obvious overfitting. This study shows that applying transfer learning in CNN can detect breast cancer from mammograms, and training a NN-classifier by feature extraction is a faster method in transfer learning.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
          
    </table>

        <h2 id="preprints">Past Researches</h2>
    
    
<div class="areatitle" id="Hyperspectral">Hyperspectral Images-based Cardiac Ablation Lesion Detection</div>
    
              <table width="961" border="0" cellpadding="2" cellspacing="10">
                  
                  
                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_jmi2018.png" alt="Results for porcine atria clustered by k-means" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

[J] <papertitle>Application of unsupervised learning to hyperspectral imaging of cardiac ablation lesions</papertitle><br />
        
        <b>Shuyue Guan</b>, Huda Asfour, Narine Sarvazyan, Murray Loew<br />
        Journal of Medical Imaging (JMI), 2018<br />
                            [<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6294845/" target="_blank">Paper</a>] <br />
                            <p></p>
[C] <papertitle>Lesion detection for cardiac ablation from auto-fluorescence hyperspectral images</papertitle><br />
        
        <b>Shuyue Guan</b>, Murray Loew, Huda Asfour, Narine Sarvazyan, Narine Muselimyan<br />
        SPIE Medical Imaging, 2018<br />
                            [<a href="https://doi.org/10.1117/12.2293652" target="_blank">Paper</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p> Atrial fibrillation is the most common cardiac arrhythmia. It is being effectively treated using the radiofrequency
ablation (RFA) procedure, which destroys culprit tissue and creates scars that prevent the spread of
abnormal electrical activity. Long-term success of RFA could be improved further if ablation lesions can be
directly visualized during the surgery. We have shown that autofluorescence-based hyperspectral imaging
(aHSI) can help to identify lesions based on spectral unmixing. We show that use of k-means clustering,
an unsupervised learning method, is capable of detecting RFA lesions without a <em>priori</em> knowledge of the
lesions’ spectral characteristics. We also show that the number of spectral bands required for successful
lesion identification can be significantly reduced, enabling the use of increased spectral bandwidth.
Together, these findings can help with clinical implementation of a percutaneous aHSI catheter, since by
reducing the number of spectral bands one can reduce hypercube acquisition and processing times, and
by increasing the spectral width of individual bands one can collect more photons. The latter is of critical
importance in low-light applications such as intracardiac aHSI. The ultimate goal of our studies is to help
improve clinical outcomes for atrial fibrillation patients.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                                    <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_boe2018.png" alt="Histogram distributions and boxplot results of accuracies for all studies" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Optimization of wavelength selection for multispectral image acquisition: a case study of atrial ablation lesions</papertitle><br />
        
        Huda Asfour, <b>Shuyue Guan</b>, Narine Muselimyan, Luther Swift, Murray Loew, Narine Sarvazyan<br />
        Biomedical Optics Express, 2018<br />
                            [<a href="https://www.osapublishing.org/boe/abstract.cfm?uri=boe-9-5-2189" target="_blank">Paper</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>In vivo autofluorescence hyperspectral imaging of moving objects can be challenging due to motion artifacts and to the limited amount of acquired photons. To address both limitations, we selectively reduced the number of spectral bands while maintaining accurate target identification. Several downsampling approaches were applied to data obtained from the atrial tissue of adult pigs with sites of radiofrequency ablation lesions. Standard image qualifiers such as the mean square error, the peak signal-to-noise ratio, the structural similarity index map, and an accuracy index of lesion component images were used to quantify the effects of spectral binning, an increased spectral distance between individual bands, as well as random combinations of spectral bands. Results point to several quantitative strategies for deriving combinations of a small number of spectral bands that can successfully detect target tissue. Insights from our studies can be applied to a wide range of applications.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                         
          
    </table>
    
    <div class="areatitle" id="FrostPoint">Climate (Frost Point) Data Collection and Analysis by R</div>

              <table width="961" border="0" cellpadding="2" cellspacing="10">
                  
                  
                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/global_frost_point.png" alt="Frost points at 12Z o'clock in Mar 09, 2010" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">
<papertitle>Project: Frost Detection</papertitle><br />
        Advisor: <a href="https://www.cs.seas.gwu.edu/claire-monteleoni" target="_blank">Claire Monteleoni</a><br />
                            June-August, 2016<br />
                            
                            [<a href="doc/fp_proj/Report1.pdf" target="_blank">Report1</a>] [<a href="doc/fp_proj/Report2.pdf" target="_blank">Report2</a>] [<a href="doc/fp_proj/Report3.pdf" target="_blank">Report3</a>] [<a href="doc/fp_proj/Report_fin.pdf" target="_blank">ReportFIN</a>] [<a href="https://github.com/ShuyueG/frost_point" target="_blank">Code</a>]

<button class="collapsible">Flowchart</button>

<div class="content">
  <p> <img src="imgs/frost_point_fc.png" alt="Results for porcine atria clustered by k-means" width="180px" class="center" /></p>
        
</div>
                      
                    </td>
                </tr>
          <!-- paper end -->
                                    
          
    </table>
    
<div class="areatitle" id="Wooden">Non-destructive Testing for Wooden Materials</div>
    
        
              <table width="961" border="0" cellpadding="2" cellspacing="10">
                  
                  
                <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                    
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_fuzzyBPNN.png" alt="FBP Flow Chart" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Wood Defects Recognition Based on Fuzzy BP Neural Network</papertitle><br />
        
        Hongbo Mu, Mingming Zhang, Dawei Qi, <b>Shuyue Guan</b>, Haiming Ni<br />
        International Journal of Smart Home, 2015<br />
                            [<a href="https://www.earticle.net/Article/A246130" target="_blank">Paper</a>] [<a href="http://dx.doi.org/10.14257/ijsh.2015.9.5.14">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Firstly, we applied the X-ray non-destructive testing technology to detect wood defects for
getting the images. After graying the images, we calculated their GLCMS(Gray Level Cooccurrence Matrixes), then we normalized GLCMS to obtain the joint probabilities of GLCMS. The feature vectors of images, which included 13 eigenvalues of images were
calculated and extracted by the joint probability of GLCMS. The fuzzy BP neural
network(abbreviated as FBP) was designed by combining fuzzy mathematics and BP neural
network . And the FBP neural network was regarded as the membership function of feature
vectors, the outputs of the network was regarded as the degree of membership to the feature
vectors in each category. We use the maximum degree of membership method for the pattern
recognition of feature vectors, so the automatic identification and classification for feature
vectors were achieved , and then the automatic identification of wood defects was realized.
By simulated study and training many times, the results shown that the average recognition
success rate of the network was more than 90%, and some FBP networks had an extremely
high recognition success rate to training samples and test samples.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                                    <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_aiss2013.png" alt="The larger number of E demonstrates the poorer quality of
tested blockboards" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Defect Edge Detection in Blockboard X-ray Images by Shannon Entropy</papertitle><br />
        
        <b>Shuyue Guan</b>, Dawei Qi<br />
        Advances in Information Sciences and Service Sciences (AISS), 2013<br />
                            [<a href="https://www.semanticscholar.org/paper/Defect-Edge-Detection-in-Blockboard-X-ray-Images-by-Guan-Qi/3390873efabd09aa20c213b5febb7aef93d60742" target="_blank">Paper</a>] [<a href="/doc/AISS2013.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>A Shannon entropy-based image processing approach is introduced and applied to the blockboard
X-ray images obtained from nondestructive scanning. X-ray nondestructive testing technology has been
applied to the detection of internal defects in blockboard. In this paper, we select a probability
distribution function to calculate the Shannon entropy in images processing. And we define a novel
Defect Edge Index (DEI) for analyzing the defect edges in images. Through studying and processing
the DEIs, the defect edges extracting is achieved. Furthermore, a new index E for quality evaluation
was also studied out by the DEIs. The number of E can demonstrate the quality of examined
blockboard. Experimental results display that the image processing method based on Shannon entropy
theory is effective and the quality evaluation index E is accurate. Thus, a promising method for
detecting and analyzing defect edges in blockboard X-ray images is provided.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->
                                       <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_ijact2012.png" alt="Redrawing defect region by lines" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Defects description in blockboard by Hough transform and Minimum-Perimeter polygons</papertitle><br />
        
        <b>Shuyue Guan</b>, Dawei Qi<br />
        International Journal of Advancements in Computing Technology (IJACT), 2012<br />
                           [<a href="http://www.scopus.com/inward/record.url?eid=2-s2.0-84871894525&partnerID=MN8TOARS" target="_blank">Paper</a>] [<a href="/doc/IJACT2012pdf.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>The blockboard defects were detected by Hough transform and Minimum-Perimeter Polygons. Xray
nondestructive testing system was used to obtain X-ray blockboard images. The rough binary
images of blockboard were got from X-ray images. An initial area of the blockboard defect was
simplified by Mathematical morphology, and then the edges of the area image were detected by
Minimum-Perimeter Polygons (MPP). Large numbers of lines around the boundary were detected by
Hough transform. The lines data were processed by the mathematics and computer graphics methods.
The results show that the blockboard defects can be described by several certain lines on the screen.
And then, the parameters of lines can be obtained by computers easily. Hence, to defects description,
the method in this paper is much more convenient and faster than the traditional methods.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->                      
                                        <!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_aiss2012.png" alt="The Singularity Index &alpha; of Image" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Multifractal Analysis of Blockboard X-Ray Images for the Defect Detection</papertitle><br />
        
        <b>Shuyue Guan</b>, Dawei Qi<br />
        Advances in Information Sciences and Service Sciences (AISS), 2012<br />
                           [<a href="http://www.scopus.com/inward/record.url?eid=2-s2.0-84867765261&partnerID=MN8TOARS" target="_blank">Paper</a>] [<a href="/doc/AISS2012.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Nowadays, nondestructive testing technology is a new subject that has gotten rapid development. Xray
nondestructive scanning technology has been applied to the detection of internal defects in
blockboard for the purpose of obtaining prior information that can be used to arrive at better
production quality. Since producers currently cannot see the inside of blockboard until its faces are
revealed by cutting. Therefore, the recognition of internal defects has become gradually significant.
The traditional Euclidean geometry is not proficient of describing different natural objects and
phenomena. In contrast, fractal geometry and its multifractal extension are new implements which can
be used for describing, processing and analyzing complex shapes and images. A method in blockboard
X-ray image defect detection based on multifractal theory was applied in this paper. The Lipschitz–H&#xF6;lder exponent &alpha; of image was computed first. Then its multifractal spectrum <em>f(&alpha;)</em> was calculated and different image points were classified by analysis of multifractal spectrum <em>&alpha;-f(&alpha;).</em> Experimental results showed that the method based on multifractal theory was effective to detect defects in
blockboard X-ray images. Due to the information of defect, the areas and boundaries was obtained
accurately by this method. Hence, in this paper, a dependable method by applying multifractal theory
in defect detection of blockboard X-ray images was provided.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->                      
<!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_amr2012.png" alt="Structure of our CT scanning system" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Application of computed tomography in wood-polymer composites density detection</papertitle><br />
        
        Yu Han, Dawei Qi, <b>Shuyue Guan</b><br />
        Advanced Materials Research (AMR), 2012<br />
                            [<a href="https://www.scientific.net/AMR.428.57" target="_blank">Paper</a>] [<a href="doc/AMR2012.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>CT technology was used in nondestructive testing procedure of Wood-plastic composite in
the paper as well as computes the CT number range of different Wood-plastic composite tomography
slices in statistic method. A fitting mathematical model between CT number and Wood-plastic
composite density was Calculated, because of the linear relationship exists between Wood-plastic
composite density and CT number. Hence, a new method in the nondestructive testing of
Wood-plastic composite density was provided.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->     
<!-- paper strat -->
                <tr>
                        <td width="180" valign="center">
                                <center>
                                        
                                    <!-- Zoom in -->
                                    <div class="img-hover-zoom">
                                        <img class="myImg" src="imgs/paper_ibi2011.png" alt="Fiberboard CT number-density relationship graph" width="180px" />
                                        </div>
                                    
                                    <!-- The full-size Modal -->
                                    <div id="myModal" class="modal">
                                      <span class="close">&times;</span>
                                      <img class="modal-content" id="img01">
                                      <div id="caption"></div>
                                    </div>

                                </center>
                        </td>

                        <td width="714" valign="top">

<papertitle>Automatic Fiberboard Density Testing Based on Application of Computed Tomography</papertitle><br />
        
       <b>Shuyue Guan</b>, Dawei Qi, Yu Han<br />
        Information and Business Intelligence (IBI), 2011<br />
                            [<a href="https://doi.org/10.1007/978-3-642-29084-8_95" target="_blank">Paper</a>] [<a href="doc/IBI2011.pdf">PDF</a>]

<button class="collapsible">Abstract</button>
<div class="content">
  <p>Fiberboard has long been a significant practical material. Computed
tomography (CT) shows great potential for nondestructive testing of the
property and internal structure of fiberboard. In the paper, utilize CT
technology in nondestructive testing procedure of fiberboard as well as
compute the CT number range of different fiberboard tomography slices in
statistic method. Therefore it provides an automatic manipulative procedure in
the fiberboard nondestructive testing based on CT. The fitting linear formula
between CT number and fiberboard density was calculated, because of the
linear relationship exists between fiberboard density and CT number. Thus, a
new method in the nondestructive testing of fiberboard defect and fiberboard
density is provided.</p>
        
</div>
                        </td>
                </tr>
          <!-- paper end -->     
    </table>




        <table border="0" cellspacing="4" cellpadding="2">
             
        </table>

<h2 id="conf">Presentations</h2>
        <h4 id="ivtalks">Invited Talks</h4>
        <ul>
            <li>Invited Speaker, FDA/CDRH/OSEL/DIDSR AI/ML Seminar, &quot;A Novel Intrinsic Measure of Data Separability – the Distance-based Separability Index (DSI) and its Applications&quot;, Online. (December 6, 2021)</li>
            <li>Invited Graduate Presentation for the GW BME Day, &quot;Introduce to an Intrinsic Measure of Data Separability – the Distance-based Separability Index (DSI)&quot;, Washington DC. (November 1, 2021)</li>
            <li>Invited Speaker, FDA/CDRH/OSEL/DIDSR Q&A and Special Topics, &quot;Analysis of Generalizability of Deep Neural Networks Based on the Complexity of Decision Boundary&quot;, Online. (May 27, 2021)</li>
        </ul>
    
<h4 id="conftalks">Conferences</h4>
        <ul>
			<li><a href="https://ivpcl.unm.edu/SSIAI2024/index.html" target="_blank">2024 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)</a>, &quot;Restorable synthesis: average synthetic segmentation converges to a polygon approximation of an object contour in medical images&quot;, Santa Fe, New Mexico (March 2024)</li>
			<li><a href="https://www.aipr-workshop.org/past-conference-information" target="_blank">52st IEEE Applied Imagery Pattern Recognition (AIPR)</a>, &quot;The training accuracy of two-layer neural networks: its estimation and understanding using random datasets&quot;, St. Louis, Missouri (September 2023)</li>
			<li><a href="https://www.aipr-workshop.org/past-conference-information" target="_blank">51st IEEE Applied Imagery Pattern Recognition (AIPR)</a>, &quot;Informing selection of performance metrics for medical image segmentation evaluation using synthetic segmentations&quot;, Washington DC. (October 2022)</li>
            <li><a href="https://www.aipr-workshop.org/past-conference-information" target="_blank">50th IEEE Applied Imagery Pattern Recognition (AIPR)</a>, &quot;A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers&quot;,  Online. (October 2021)</li>
            <li><a href="https://www.icmla-conference.org/icmla20/" target="_blank">19th IEEE International Conference on Machine Learning and Applications (ICMLA)</a>, &quot;Analysis of Generalizability of Deep Neural Networks Based on the Complexity of Decision Boundary&quot;, Online. (December 2020)</li>
            <li><a href="https://ictai2020.org/" target="_blank">32nd IEEE International Conference on Tools with Artificial Intelligence (ICTAI)</a>, &quot;An Internal Cluster Validity Index Using a Distance-based Separability Measure&quot;, Online. (November 2020)</li>
            <li><a href="https://www.aipr-workshop.org/past-conference-information" target="_blank">49th IEEE Applied Imagery Pattern Recognition (AIPR)</a>, &quot;Understanding the Ability of Deep Neural Networks to Count Connected Components in Images&quot;,  Online. (October 2020)</li>
            <li><a href="https://www.aipr-workshop.org/past-conference-information" target="_blank">48th IEEE Applied Imagery Pattern Recognition (AIPR)</a>, &quot;Evaluation of Generative Adversarial Network Performance Based on Direct Analysis of Generated Images&quot;, Washington DC. (October 2019)</li>
            <li><a href="https://spie.org/conferences-and-exhibitions/past-conferences-and-exhibitions/medical-imaging-2019" target="_blank">2019 SPIE Medical Imaging Conference</a>, &quot;Using Generative Adversarial Networks and Transfer Learning for Breast Cancer Detection by Convolutional Neural Networks&quot;, San Diego CA. (February 2019)</li>
            <li><a href="https://www.aipr-workshop.org/past-conference-information" target="_blank">47th IEEE Applied Imagery Pattern Recognition (AIPR)</a>, &quot;Segmentation of Thermal Breast Images Using Convolutional and Deconvolutional Neural Networks&quot;,  Washington DC. (October 2018)</li>
            <li><a href="http://iwbi2018.synchrosystems.com/" target="_blank">14th International Workshop on Breast Imaging (IWBI)</a>, &quot;Breast Cancer Detection Using Synthetic Mammograms from Generative Adversarial Networks in Convolutional Neural Networks&quot;, Atlanta GA. (July 2018)</li>
            <li><a href="https://biomedicalimaging.org/2018/" target="_blank">15th IEEE International Symposium on Biomedical Imaging (ISBI)</a>, &quot;Breast Cancer Detection Using Transfer Learning in Convolutional Neural Networks&quot;,  Washington DC. (April 2018)</li>
            <li><a href="https://spie.org/conferences-and-exhibitions/past-conferences-and-exhibitions/medical-imaging-2018" target="_blank">2018 SPIE Medical Imaging Conference</a>, &quot;Lesion Detection for Cardiac Ablation from Auto-fluorescence Hyperspectral Images&quot;, Houston TX. (February 2018)</li>
            <li><a href="https://www.aipr-workshop.org/past-conference-information" target="_blank">46th IEEE Applied Imagery Pattern Recognition (AIPR)</a>, &quot;Breast Cancer Detection Using Transfer Learning in Convolutional Neural Networks&quot;, Washington DC. (October 2017)</li>
        </ul>
    
<h4 id="othertalks">Others</h4>
        <ul>
            <li>GW SEAS R&amp;D Showcase, &quot;A Novel Intrinsic Measure of Data Separability&quot;. (April 2022)</li>
            <li>Special Topic Lecture in Lab Meeting, &quot;Optimization and Troubleshooting in Machine Learning&quot;. (April 2022) [<a href="https://cpb-us-e1.wpmucdn.com/blogs.gwu.edu/dist/f/464/files/2022/04/Shuyue_Optimization-and-Troubleshooting-in-Machine-Learning.pdf">PDF</a>]</li>
            <li>GW Research Showcase, &quot;A Novel Intrinsic Measure of Data Separability&quot;. (April 2022)</li>
            <li>Special Topic Lecture in Pattern Recognition and Machine Learning course, &quot;Software Tools for Machine Learning & Deep Learning&quot;. (September 2021) [<a href="https://cpb-us-e1.wpmucdn.com/blogs.gwu.edu/dist/f/464/files/2021/10/Shuyue_Software-tools-for-ML-and-DL.pdf">PDF</a>]</li>
            <li>GW Research Showcase, &quot;A Novel Measure to Evaluate Generative Adversarial Networks Based on Direct Analysis of Generated Images&quot;. (April 2021)</li>
            <li>Special Topic Lecture in Lab Meeting, &quot;High Performance Computing in GWU&quot;. (November 2020) [<a href="https://cpb-us-e1.wpmucdn.com/blogs.gwu.edu/dist/f/464/files/2020/11/mia_GW_HPC_intro.pdf">PDF</a>]</li>
            <li>GW BME Day, &quot;Evaluation of Generative Adversarial Network Performance Based on Direct Analysis of Generated Images&quot;. (November 2019)</li>
            <li>GW SEAS R&amp;D Showcase, &quot;Evaluation of Generative Adversarial Network Performance Based on Direct Analysis of Generated Images&quot;. (October 2019)</li>
            <li>GW Research Showcase, &quot;Can a Convolutional Neural Network Implement Histogram Equalization in Image Analysis?&quot;. (April 2019)</li>
            <li>GW BME Day, &quot;Segmentation of Thermal Breast Images Using Convolutional and Deconvolutional Neural Networks&quot;. (November 2018)</li>
            <li>GW Research Showcase, &quot;Breast Cancer Detection Using Transfer Learning in Convolutional Neural Networks&quot;. (April 2018)</li>
            <li>GW SEAS R&amp;D Showcase, &quot;Breast Cancer Detection Using Transfer Learning in Convolutional Neural Networks&quot;. (February 2018)</li>
            <li>GW Research Showcase, &quot;Toward Real-time Lesion Detection for Cardiac Ablation from Auto-fluorescence Hyperspectral Images&quot;. (April 2017)</li>
            <li>GW SEAS R&amp;D Showcase, &quot;Lesion Detection for Cardiac Ablation from Auto-fluorescence Hyperspectral Images&quot;. (February 2017)</li>
        </ul>

    
        <h2 id="talks">Academic Service</h2>
    
   <a href="https://www.webofscience.com/wos/author/record/GQQ-3858-2022" target="_blank"><img src="imgs/Clarivate-logo.png" border="0" style="margin-left:35px;float: right" height="40px"/></a>
    
    <ul>
      <li>  Journal Reviewer: <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">IEEE&nbsp;TPAMI</a>,
                              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385" target="_blank">IEEE&nbsp;TNNLS</a>,
                              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4235" target="_blank">IEEE&nbsp;TEVC</a>,
                              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42" target="_blank">IEEE&nbsp;TMI</a>,
                              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6687317" target="_blank">IEEE&nbsp;TBD</a>,
                              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6488907" target="_blank">IEEE&nbsp;IoT</a>,
                              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6287639" target="_blank">IEEE&nbsp;Access</a>,
		  					  <a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging" target="_blank">SPIE&nbsp;JMI</a>,
		  					  <a href="https://dl.acm.org/journal/tkdd" target="_blank">ACM&nbsp;TKDD</a>,
                              <a href="https://ees.elsevier.com/pr/" target="_blank">PR</a>,
                              <a href="https://www.journals.elsevier.com/international-journal-of-medical-informatics" target="_blank">IJMI</a>,                   <a href="https://www.journals.elsevier.com/expert-systems-with-applications" target="_blank">ESWA</a>,
		  					  <a href="https://www.sciencedirect.com/journal/chaos-solitons-and-fractals" target="_blank">CSF</a>,
		  					  <a href="https://www.springer.com/journal/12652/" target="_blank">JAIHC</a>,
		  					  <a href="https://www.hindawi.com/journals/cin/" target="_blank">CIN</a>,
		  					  <a href="https://www.hindawi.com/journals/mpe/" target="_blank">MPE</a>
                              <a href="https://onlinelibrary.wiley.com/journal/14678640" target="_blank">Computational&nbsp;Intelligence</a>,
                              <a href="https://www.worldscientific.com/worldscinet/bme" target="_blank">BME</a>
                              
		  					  
        </li>
        <li>
            Conference Reviewer: <a href="https://spie.org/conferences-and-exhibitions/medical-imaging" target="_blank">SPIE MI 2024</a>, <a href="http://2023.biomedicalimaging.org/en/" target="_blank">ISBI 2023</a>,
                                WACV <a href="http://wacv2021.thecvf.com/home" target="_blank">2021</a>/<a href="https://wacv2022.thecvf.com/" target="_blank">2022, <a href="http://aistats.org/aistats2021/" target="_blank">AISTATS&nbsp;2021</a></a>
            </li>
		<li>Program Committee of 2024 <a href="https://spie.org/mi/conferencedetails/medical-image-processing" target="_blank">SPIE Medical Imaging - Image Processing Conference</a></li>
		<li>Guest Editor of the Special Issue on <i>Diagnostics</i> (ISSN 2075-4418): <a href="https://www.mdpi.com/journal/diagnostics/special_issues/C13KN8YK2V" target="_blank">Deep Learning in Medical and Biomedical Image Processing</a></li> Deadline for manuscript submissions: <b>31 December 2024</b>
		<li>Session Chair for 2024 SPIE Medical Imaging - Image Processing Conference: Session 9 (Explainable and Trustworthy AI), San Diego CA. (February 2024)</li>
        <li>Session Chair for 2022 SPIE Medical Imaging - Image Processing Conference: Session 4 (Classification and Detection) and Session 7 (Segmentation II), San Diego CA. (February 2022)</li>
     </ul>    
    

<h2 id="teaching">Teaching Assistant</h2>

        <ul>
            <li>BME 4925W: Biomedical Engineering Capstone Project Lab III (Spring 2021)</li>
            <li>BME/ECE 6885: Computer Vision (Fall 2020, Spring 2018)</li>
            <li>BME 2820: Biomedical Engineering Programming I [MATLAB Programming] (Spring 2020, Fall 2019, Spring 2019)</li>
            <li>BME 2825: Biomedical Engineering Programming II [C Programming] (Fall 2019)</li>
            <li>BME/ECE 6850: Pattern Recognition (Fall 2018, Fall 2017)</li>
            <li>BME 3915W: Biomedical Engineering Capstone Project Lab I (Spring 2018)</li>
            <li>BME/ECE  6840: Digital Image Processing (Spring 2017)</li>
            <li>CSCI 6212: Design and Analysis of Algorithms (Fall 2016)</li>
            <li>CSCI 3362/6362: Probability for Computer Science (Spring 2016)</li>
        </ul>
    
            <h2 id="volunteer">Volunteer</h2>

        <ul>
               
            <li>The 24th International Conference on Artificial Intelligence and Statistics (2021)</li>
            <li>IEEE Applied Imagery Pattern Recognition Workshop (2019, 2018, 2017, 2016)</li>
            <li>IEEE International Symposium on Biomedical Imaging Conference (2018)</li>

        </ul>

    
<!--

        <h2 id="services">Services</h2>


        <ul>
</ul>


        <h2 id="honors">Honors &amp; Awards</h2>


        <table style="border-spacing:2px">
       
        </table>


        <h2 id="miscellany">Miscellany</h2>


 -->         

        <div id="footer">
                <div id="footer-text">
                </div>
            <p align="right">Last Updated by Shuyue Guan: <span id="demo"></span>.</p>

            <script>
            document.getElementById("demo").innerHTML = document.lastModified;
            </script>
        </div>
 
	    <script src="js/abstract.js"></script>
        <script src="js/full_img.js"></script>
</body>
</html>
